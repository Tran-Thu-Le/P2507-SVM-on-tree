\section{Classical SVM}
\label{sec:classical_svm}

In this section, we review the fundamental concepts of Support Vector Machines that serve as the theoretical foundation for our tree-based extension.

\subsection{Hard margin SVM}


Given a labeled training dataset $\{(x_i, y_i)\}_{i=1}^n$ where $x_i \in \mathbb{R}^d$ are feature vectors and $y_i \in \{-1, +1\}$ are binary class labels, the goal of SVM is to find a hyperplane, say $w^T x + b =0$ for $w \in \RR^d$ and $b \in \RR$, that optimally separates the two classes, in the sense that 

\begin{align*}
    w^T x + b & \geq 1, \quad \forall y_i =+1\\
    w^T x + b & \leq -1, \quad \forall y_i =-1
\end{align*}

The margin is defined as the distance between the two hyperplanes $w^T x + b=1$ and $w^T x + b=-1$, which is $\frac{2}{\|w\|_2}$. SVM model seeks the separating hyperplane with maximal margin.
This leads to the following optimization problem:

\begin{align}
\min_{w,b} \quad &\frac{1}{2}\|w\|^2 \\
\text{s.t.} \quad &y_i(w^T x_i + b) \geq 1, \quad i = 1,\ldots,n
\end{align}

The constraints ensure that all training points are correctly classified with at least unit distance from the decision boundary.

The point (or equivalently considered as a vector) $s\in V_+$ and $p\in V_+$ are said to be positive and negative support vector, respectively, if they are on the supporting hyperplane, \ie
\begin{align*}
    w^s + b &= 1\\
    w^p + b &= -1.\\
\end{align*}
Hence the name SVM.


\subsection{Soft Margin SVM}

To handle non-separable cases, the soft margin SVM introduces slack variables $\xi_i \geq 0$:

\begin{align}
\min_{w,b,\xi} \quad &\frac{1}{2}\|w\|^2 + C\sum_{i=1}^n \xi_i \\
\text{s.t.} \quad &y_i(w^T x_i + b) \geq 1 - \xi_i, \quad i = 1,\ldots,n \\
&\xi_i \geq 0, \quad i = 1,\ldots,n
\end{align}

where $C > 0$ is the regularization parameter that balances between margin maximization and training error minimization.



\begin{remark}[Kernel method]
    To handle nonlinear classification, SVM can be extended using kernel functions $K(x_i, x_j) = \phi(x_i)^T \phi(x_j)$ that implicitly map data to higher-dimensional feature spaces. Common kernels include:

    \begin{itemize}
    \item \textbf{Linear kernel}: $K(x_i, x_j) = x_i^T x_j$
    \item \textbf{RBF kernel}: $K(x_i, x_j) = \exp(-\gamma \|x_i - x_j\|^2)$
    \item \textbf{Polynomial kernel}: $K(x_i, x_j) = (x_i^T x_j + c)^p$
    \end{itemize}
\end{remark}

\section{New interpretation of SVM based on support vectors}

In this section, we rewrite the soft-margin SVM model in term of support vectors. This will lay the foundation for our SVM model on tree in the next section. 

Define 
\begin{equation*}
    \sigma(x_i) = w^T x_i + b
\end{equation*}
and 
\begin{equation*}
    d(x_i, x_j) = \sigma(x_i) - \sigma(x_j)
\end{equation*}

Let $s \in V_+$ and $p \in V_-$ be two support vectors, then the margin is given by the distance between $s$ and $p$, 
\begin{equation*}
    \frac{2}{\|w\|_2}
    = \frac{(w^T s + b) - (w^T p + b)}{\|w\|_2}
    = \frac{d(s, p)}{\|w\|_2}.
\end{equation*} 

In the soft margin SVM, the noise can also be written in terms of $s$ and $p$ as follows. For $x_i \in V_+$, we should have 
\begin{align*}
    \xi_i & = 1 - (w^T x_i + b), \forall x_i \in V_+\\
    & = \sigma(s)  - \sigma(x_i)\\
    & = d(s, x_i)\\
\end{align*}




