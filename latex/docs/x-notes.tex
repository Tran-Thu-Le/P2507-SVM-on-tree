\section{Notes}
\remLE{Please read this section first. This section will provide the main notations used in this paper. This section will be removed when we finish the project.}



\textbf{Notations.} 
We will use the following notations for SVM model on $\RR^n$
\begin{enumerate}
    \item Training data $(x_i, y_i) \in \RR^d \times \{-1, 1\}$ for $i=1,..., n$
    \item Classical soft-margin SVM model 
    \begin{equation*}
        \min_{w\in \RR^d, b \in \RR} \quad 
        \sum_{i=1}^n \max(1- y_i (w^T x_i + b), 0) + \frac{\lambda}{2}\norm{w}_2^2
    \end{equation*}
\end{enumerate}

For the SVM model on tree $\tree$, we will use the following notations:
\begin{enumerate}
    \item Training data $(x_i, y_i) \in \RR^d \times \{-1, 1\}$ for $i=1,..., n$
    \item Denote $V_+=\{x_i \in V: y_i = +1\}, V_-=\{x_i \in V: y_i = -1\}$
    \item Let $\Delta$ be the line passing through the centers of $V_+$ and $V_-$
    \item Let $x'_i$ be the projection of $x_i$ onto $\Delta$
    \item Tree $\tree = (V, E)$ be a tree with vertex set $V=\{x_1, ...., x_m, x'_1,..., x'_n\} \subset \RR^n$ and edge set $E= \{(x'_i , x'_{i+1}): i=1,..., n-1\} \cup \{(x_i, x'_i): i =1,..., n\}$
    \item The SVM on tree aims to minimize the objective function 
    \begin{align*}
        \min_{u \in V_+, v \in V_-} \quad & P(u, v) = f(u, v) - \lambda d(u, v)
    \end{align*}
    where $d(u, v)$ stands for the margin of the model and $f$ is the loss function defined as $f(u, v) = f_+(u, v) + f_-(u, v)$ with
    \begin{equation*}
        f_+(u, v) = \sum_{x \in V_+(u, v)} d(x, u)
        \text{ and }
        f_-(u, v) = \sum_{x \in V_-(v, u)} d(x, v).
    \end{equation*}
    Here, $V_+(u, v)$ is the set of $+1$-labelled vertices of subtree rooted at $u$ containing $v$ and $V_-(v, u)$ is the set of $-1$-labelled vertices of subtree rooted at $v$ containing $u$.

    \item The separating hyperplane is defined as the bisector of the optimal pair of support vectors.

\end{enumerate}