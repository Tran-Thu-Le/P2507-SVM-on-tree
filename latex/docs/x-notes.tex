\section{Notes}
\remLE{This section provides the main notations used in this paper. This section will be removed when we finish the project.}



\textbf{Notations.} 
We will use the following notations for SVM model on $\RR^n$
\begin{enumerate}
    \item Training data $(x_i, y_i) \in \RR^d \times \{-1, 1\}$ for $i=1,..., n$
    \item Classical soft-margin SVM model 
    \begin{equation*}
        \min_{w\in \RR^d, b \in \RR} \quad 
        \sum_{i=1}^n \max(1- y_i (w^T x_i + b), 0) + \frac{\lambda}{2}\norm{w}_2^2
    \end{equation*}
\end{enumerate}

For the SVM model on tree $\tree$, we will use the following notations:
\begin{enumerate}
    \item Training data $(x_i, y_i) \in \RR^d \times \{-1, 1\}$ for $i=1,..., n$
    \item Denote $V_+=\{x_i \in V: y_i = +1\}, V_-=\{x_i \in V: y_i = -1\}$
    \item Let $\Delta$ be the line passing through the centers of $V_+$ and $V_-$
    \item Let $x'_i$ be the projection of $x_i$ onto $\Delta$
    \item Tree $\tree = (V, E)$ be a tree with vertex set $V=\{x_1, ...., x_m, x'_1,..., x'_n\} \subset \RR^n$ and edge set $E= \{(x'_i , x'_{i+1}): i=1,..., n-1\} \cup \{(x_i, x'_i): i =1,..., n\}$
    \item The SVM on tree aims to minimize the objective function 
    \begin{align*}
        \min_{u \in V_+, v \in V_-} \quad & P(u, v) = f(u, v) - \lambda d(u, v)
    \end{align*}
    where $d(u, v)$ stands for the margin of the model and $f$ is the loss function defined as $f(u, v) = f_+(u, v) + f_-(u, v)$ with
    \begin{equation*}
        f_+(u, v) = \sum_{x \in V_+(u, v)} d(x, u)
        \text{ and }
        f_-(u, v) = \sum_{x \in V_-(v, u)} d(x, v).
    \end{equation*}
    Here, $V_+(u, v)$ is the set of $+1$-labelled vertices of subtree rooted at $u$ containing $v$ and $V_-(v, u)$ is the set of $-1$-labelled vertices of subtree rooted at $v$ containing $u$.

    \item The separating hyperplane is defined as the bisector of the optimal pair of support vectors.

\end{enumerate}


\section{Short Intro}

\remLE{This section shows the main results and will be removed.}

\begin{itemize}
    \item SVM is important 
    \item Solving it is time consuming
    \item There are methods to accelerate the resolution 
    \item First approach is to use multiple projections of data points onto 1D space, leading to an SVM on a path. See paper: Solving Linear SVMs with Multiple 1D Projections
    \item Another reasearch direction is to used tree to approximate data points, but for solving Optimal Transport not SVM. See paper: Tree-Sliced Variants of Wasserstein Distances
    \item In this paper, we propose a way to use tree to approximate data points for solving SVM 
    \item After formulize the problem. We develope algorithms for solving the problem. The naive approach solve the problem in $O(n^3)$. We propose a dynamic method to reduce the complexity to $O(n^2)$. For unit parameter, we prove a theoretical property and solve the problem in $O(n \log n)$.
\end{itemize}