\section{Classical SVM}
\label{sec:classical_svm}

In this section, we review the fundamental concepts of Support Vector Machines that serve as the theoretical foundation for our tree-based extension.

\subsection{Problem Formulation}

Given a labeled training dataset $\{(x_i, y_i)\}_{i=1}^n$ where $x_i \in \mathbb{R}^d$ are feature vectors and $y_i \in \{-1, +1\}$ are binary class labels, the goal of SVM is to find a hyperplane that optimally separates the two classes.

The decision function is defined as:
\[
f(x) = \text{sign}(w^T x + b)
\]
where $w \in \mathbb{R}^d$ is the weight vector normal to the hyperplane and $b \in \mathbb{R}$ is the bias term.

\subsection{Margin Maximization}

The key insight of SVM is to maximize the margin $\gamma = \frac{2}{\|w\|}$ between the classes. This leads to the following optimization problem:

\begin{align}
\min_{w,b} \quad &\frac{1}{2}\|w\|^2 \\
\text{s.t.} \quad &y_i(w^T x_i + b) \geq 1, \quad i = 1,\ldots,n
\end{align}

The constraints ensure that all training points are correctly classified with at least unit distance from the decision boundary.

\subsection{Soft Margin SVM}

To handle non-separable cases, the soft margin SVM introduces slack variables $\xi_i \geq 0$:

\begin{align}
\min_{w,b,\xi} \quad &\frac{1}{2}\|w\|^2 + C\sum_{i=1}^n \xi_i \\
\text{s.t.} \quad &y_i(w^T x_i + b) \geq 1 - \xi_i, \quad i = 1,\ldots,n \\
&\xi_i \geq 0, \quad i = 1,\ldots,n
\end{align}

where $C > 0$ is the regularization parameter that balances between margin maximization and training error minimization.

\subsection{Dual Formulation}

Using Lagrangian duality, the SVM optimization can be reformulated as:

\begin{align}
\max_{\alpha} \quad &\sum_{i=1}^n \alpha_i - \frac{1}{2}\sum_{i,j=1}^n \alpha_i \alpha_j y_i y_j x_i^T x_j \\
\text{s.t.} \quad &0 \leq \alpha_i \leq C, \quad i = 1,\ldots,n \\
&\sum_{i=1}^n \alpha_i y_i = 0
\end{align}

The training points with $\alpha_i > 0$ are called \textbf{support vectors}, and they determine the decision boundary. The decision function becomes:
\[
f(x) = \text{sign}\left(\sum_{i \in SV} \alpha_i y_i x_i^T x + b\right)
\]

\subsection{Kernel Methods}

To handle nonlinear classification, SVM can be extended using kernel functions $K(x_i, x_j) = \phi(x_i)^T \phi(x_j)$ that implicitly map data to higher-dimensional feature spaces. Common kernels include:

\begin{itemize}
\item \textbf{Linear kernel}: $K(x_i, x_j) = x_i^T x_j$
\item \textbf{RBF kernel}: $K(x_i, x_j) = \exp(-\gamma \|x_i - x_j\|^2)$
\item \textbf{Polynomial kernel}: $K(x_i, x_j) = (x_i^T x_j + c)^p$
\end{itemize}

The dual formulation becomes:
\[
\max_{\alpha} \sum_{i=1}^n \alpha_i - \frac{1}{2}\sum_{i,j=1}^n \alpha_i \alpha_j y_i y_j K(x_i, x_j)
\]

While kernel methods provide flexibility for complex decision boundaries, they rely on predefined similarity measures that may not capture domain-specific structural relationships, motivating our tree-based approach.


