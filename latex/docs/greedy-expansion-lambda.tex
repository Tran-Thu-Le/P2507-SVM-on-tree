% Standalone note (does not affect the main paper)
\documentclass[10pt]{article}
\usepackage[a4paper,margin=1in]{geometry}
\usepackage{amsmath,amssymb,amsthm}
\usepackage{hyperref}

\title{Greedy Boundary Expansion for $\lambda>1$ on a Path: A Standalone Proof Note}
\author{Standalone note for internal understanding}
\date{\today}

% Theorem environments
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{corollary}{Corollary}
\newtheorem{remark}{Remark}

\begin{document}
\maketitle

\begin{abstract}
We present a clean, self-contained proof that the greedy "expand the boundary that decreases the loss the most; stop when neither expansion helps" strategy is correct for a natural class of losses when selecting an interval $[i,j]$ on a fixed path. The note is independent from the main paper and can be compiled on its own.
\end{abstract}

\section{Setting}
Let $P=(v_1,\ldots,v_k)$ be the (fixed) path connecting two support vertices. Any candidate solution corresponds to an interval $[i,j]$ with $1\le i\le j\le k$. For a parameter $\lambda>1$, consider a loss of the form
\begin{equation}\label{eq:loss}
L_\lambda(i,j)\;=\;\sum_{u\in V} \varphi_u\big(\operatorname{dist}(u,[i,j])\big)\; +\; \lambda\,\psi(j-i),
\end{equation}
where $\operatorname{dist}(u,[i,j])$ is the graph distance from $u$ to the interval $[i,j]$ on $P$ (zero if $u\in[i,j]$), each $\varphi_u:\mathbb{R}_{\ge 0}\to\mathbb{R}$ is convex and nondecreasing, and $\psi:\mathbb{R}_{\ge 0}\to\mathbb{R}$ is convex. These mild assumptions cover hinge-type or Huber-type terms on distances plus a convex length penalty.

Define the one-step expansions and their deltas:
\begin{align*}
\Delta_\ell(i\mid j) &:= L_\lambda(i-1,j)-L_\lambda(i,j) &&\text{(expand left boundary)}\\
\Delta_r(j\mid i) &:= L_\lambda(i,j+1)-L_\lambda(i,j) &&\text{(expand right boundary)}.
\end{align*}
The greedy rule chooses the boundary with the smaller (more negative) delta; it stops when both $\Delta_\ell(i\mid j)\ge 0$ and $\Delta_r(j\mid i)\ge 0$.

\section{Key structural lemmas}
\begin{lemma}[Discrete convexity in each coordinate]\label{lem:disc-conv}
Fix $j$. The function $f(i):=L_\lambda(i,j)$ is discretely convex in $i$, i.e., $f(i-1)-2f(i)+f(i+1)\ge 0$ wherever defined. Symmetrically, for fixed $i$, $g(j):=L_\lambda(i,j)$ is discretely convex in $j$.
\end{lemma}
\begin{proof}[Sketch]
For a fixed $j$, $h_u(i):=\operatorname{dist}(u,[i,j])$ is a piecewise-linear (on $\mathbb{Z}$) "V-shaped" function of $i$: it is $0$ when $i\le \mathrm{pos}(u)\le j$ and increases linearly with slope that does not decrease as $i$ moves away. Hence $h_u$ is discretely convex. Since $\varphi_u$ is convex and nondecreasing, the composition $\varphi_u\circ h_u$ is discretely convex; summing over $u$ preserves discrete convexity. Finally, $i\mapsto \lambda\,\psi(j-i)$ is convex (composition of convex with affine), hence discretely convex. The proof for $j$ is identical by symmetry.
\end{proof}

\begin{corollary}[Monotonicity of one-step deltas]\label{cor:delta-monotone}
As $i$ decreases (expanding further to the left), the first difference $\Delta_\ell(i\mid j)=f(i-1)-f(i)$ is nondecreasing; in particular, $\Delta_\ell(i-1\mid j)\ge \Delta_\ell(i\mid j)$. As $j$ increases (expanding further to the right), $\Delta_r(j\mid i)=g(j+1)-g(j)$ is nondecreasing; in particular, $\Delta_r(j+1\mid i)\ge \Delta_r(j\mid i)$.
\end{corollary}
\begin{proof}
For a discretely convex function, forward (or backward) first differences are nondecreasing. Apply Lemma~\ref{lem:disc-conv} to $f$ and $g$.
\end{proof}

\section{Optimality of the stopping criterion and greedy correctness}
\begin{theorem}[Stopping criterion implies optimality over monotone expansions]\label{thm:stop-opt}
If at $(i,j)$ we have $\Delta_\ell(i\mid j)\ge 0$ and $\Delta_r(j\mid i)\ge 0$, then for any $(i',j')$ reachable by a finite sequence of left/right expansions (i.e., $i'\le i$ and $j'\ge j$),
\[
L_\lambda(i',j')-L_\lambda(i,j)\;\ge\;0.
\]
\end{theorem}
\begin{proof}
Any $(i',j')$ in the expansion cone is obtained via a finite chain of unit expansions. The total change in loss is the sum of the corresponding one-step deltas along the chain. By Corollary~\ref{cor:delta-monotone}, each subsequent left-step delta is at least $\Delta_\ell(i\mid j)\ge 0$, and each subsequent right-step delta is at least $\Delta_r(j\mid i)\ge 0$. Summing nonnegative terms yields a nonnegative total change.
\end{proof}

\begin{theorem}[Greedy expansion is correct and terminates]\label{thm:greedy}
Starting from any $(i_0,j_0)$, repeatedly expand the boundary with the smaller (more negative) delta while $\min\{\Delta_\ell(i\mid j),\Delta_r(j\mid i)\}<0$, and stop when both deltas are nonnegative. The algorithm terminates in finitely many steps and the final $(i^*,j^*)$ satisfies the condition of Theorem~\ref{thm:stop-opt}, hence is optimal among all monotone expansions from $(i_0,j_0)$.
\end{theorem}
\begin{proof}
Each step strictly decreases the loss by at least $-\min\{\Delta_\ell,\Delta_r\}>0$. Since the length penalty $\lambda\,\psi(j-i)$ is convex in the interval length, the deltas increase along expansions (Corollary~\ref{cor:delta-monotone}) and eventually become nonnegative, so the process stops. At termination, Theorem~\ref{thm:stop-opt} applies.
\end{proof}

\begin{remark}[Towards global optimality]
If the array $L_\lambda(i,j)$ satisfies the Monge (quadrangle) inequality
\[
L_\lambda(i,j)+L_\lambda(i+1,j+1)\;\le\;L_\lambda(i+1,j)+L_\lambda(i,j+1)\quad (i<j),
\]
then $L_\lambda$ is $L^\natural$-convex on the index lattice, and local optimality with respect to unit moves implies global optimality. Verifying Monge for a specific instantiation of \eqref{eq:loss} can be done directly from preprocessed count/distance formulas.
\end{remark}

\section{What to verify in your concrete model}
To specialize the above to your exact loss, (i) write explicit one-step deltas $\Delta_\ell,\Delta_r$ using the preprocessed counts and distance sums on the path boundary layers; (ii) check the discrete second differences in $i$ and in $j$ are nonnegative (Lemma~\ref{lem:disc-conv}); (iii) optionally, check the Monge inequality if a global optimality guarantee is desired beyond monotone expansions.

\section*{Appendix: Explicit $\Delta$ formulas in the paper's notation}
This appendix mirrors the notation used in your paper and gives closed forms for one-step deltas.

\paragraph{Notation recap (from your paper).}
Spine vertices: $x'_1,\dots,x'_n$ with labels $y_k\in\{-1,+1\}$. Tree distance is $d(\cdot,\cdot)$. Preprocessed quantities:
\begin{align*}
N^{>i}_\pm &= 2\,|\{j>i: y_j=\pm1\}|, & N^{\le i}_\pm &= 2\,|\{j\le i: y_j=\pm1\}|,\\
d^{>i}_\pm &= \sum_{\substack{j>i\\ y_j=\pm1}}\big(d(x'_i,x'_j)+d(x'_i,x_j)\big), &
d^{\le i}_\pm &= \sum_{\substack{j\le i\\ y_j=\pm1}}\big(d(x'_j,x'_{i+1})+d(x_j,x'_{i+1})\big).
\end{align*}
Loss is $L_\lambda(u,v)=f(u,v)-\lambda\,d(u,v)$.

\paragraph{General pair $(x'_i,x'_j)$, $i<j$, $y_i\ne y_j$.} Consistent with your preprocessing, we use
\begin{align*}
	ext{if }(y_i,y_j)=(-1,+1):&& f(x'_i,x'_j)&=d^{>i}_-+d^{\le(j-1)}_+,\\
	ext{if }(y_i,y_j)=(+1,-1):&& f(x'_i,x'_j)&=d^{>i}_++d^{\le(j-1)}_-.
\end{align*}

\paragraph{One-step expansions and deltas.} Define
\begin{align*}
\Delta_{\text{left}}(i\mid j)&:=L_\lambda(x'_{i-1},x'_j)-L_\lambda(x'_i,x'_j), &
\Delta_{\text{right}}(j\mid i)&:=L_\lambda(x'_i,x'_{j+1})-L_\lambda(x'_i,x'_j).
\end{align*}
Each delta splits as ``noise change'' minus $\lambda$ times the change of margin. On the spine,
\begin{align*}
d(x'_{i-1},x'_j)-d(x'_i,x'_j)&=d(x'_{i-1},x'_i), & d(x'_i,x'_{j+1})-d(x'_i,x'_j)&=d(x'_j,x'_{j+1}).
\end{align*}

\noindent Case A: $(y_i,y_j)=(-1,+1)$. Then $f(x'_i,x'_j)=d^{>i}_-+d^{\le(j-1)}_+$ and
\begin{align*}
\Delta_{\text{left}}(i\mid j)
&=\big[d^{>(i-1)}_- - d^{>i}_-\big] - \lambda\,d(x'_{i-1},x'_i)\\
&=\big[ N^{>i}_-\,d(x'_{i-1},x'_i) + 2\,d(x'_{i-1},x'_i) + d(x_i,x'_i) \big] - \lambda\,d(x'_{i-1},x'_i)\\
&=\big(N^{>i}_-+2-\lambda\big)\,d(x'_{i-1},x'_i) + d(x_i,x'_i),\\[6pt]
\Delta_{\text{right}}(j\mid i)
&=\big[d^{\le j}_+ - d^{\le(j-1)}_+\big] - \lambda\,d(x'_j,x'_{j+1})\\
&=\big[ N^{\le(j-1)}_+\,d(x'_j,x'_{j+1}) + 2\,d(x'_j,x'_{j+1}) + d(x_j,x'_j) \big] - \lambda\,d(x'_j,x'_{j+1})\\
&=\big(N^{\le(j-1)}_++2-\lambda\big)\,d(x'_j,x'_{j+1}) + d(x_j,x'_j).
\end{align*}
The bracketed increments follow directly from your $O(1)$ update rule for $d^{\le\cdot}_\pm$ (mirrored to the right side) and the fact that crossing index $i$ (resp. $j$) adds the spoke $d(x_i,x'_i)$ (resp. $d(x_j,x'_j)$) and one more spine segment.

\noindent Case B: $(y_i,y_j)=(+1,-1)$. Then $f(x'_i,x'_j)=d^{>i}_++d^{\le(j-1)}_-$ and
\begin{align*}
\Delta_{\text{left}}(i\mid j)
&=\big[d^{>(i-1)}_+ - d^{>i}_+\big] - \lambda\,d(x'_{i-1},x'_i)\\
&=\big(N^{>i}_++2-\lambda\big)\,d(x'_{i-1},x'_i) + d(x_i,x'_i),\\[6pt]
\Delta_{\text{right}}(j\mid i)
&=\big[d^{\le j}_- - d^{\le(j-1)}_-\big] - \lambda\,d(x'_j,x'_{j+1})\\
&=\big(N^{\le(j-1)}_-+2-\lambda\big)\,d(x'_j,x'_{j+1}) + d(x_j,x'_j).
\end{align*}

\paragraph{Monotonicity and stopping.} As $i$ decreases, $N^{>i}_\pm$ is nondecreasing and the added terms are nonnegative, hence $\Delta_{\text{left}}$ is nondecreasing along successive left expansions. Symmetrically, as $j$ increases, $N^{\le(j-1)}_\pm$ is nondecreasing, so $\Delta_{\text{right}}$ is nondecreasing. Therefore, if at $(i,j)$ both deltas are $\ge 0$, every further expansion has nonnegative delta, so no monotone expansion can improve the loss. Greedy expansion (pick the more negative delta) strictly decreases the loss until both deltas become nonnegative.

\end{document}
