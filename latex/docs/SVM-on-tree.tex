\section{SVM on Tree}
\label{sec:svm_on_tree}
\label{sec:svm_tree}

In this section, we present a novel approach to binary classification using Support Vector Machines (SVM) on tree structures. Our method constructs an augmented tree from the data and formulates the classification problem as an optimization over support pairs on this tree.

\subsection{Problem Formulation}

\subsubsection{Data Representation and Spine Construction}

Given a labeled dataset $\{(x_i,y_i)\}_{i=1}^n \subset \mathbb{R}^d \times \{-1,+1\}$, we define the class means as:
\begin{align}
\mu_- &= \frac{1}{|V_-|}\sum_{y_i=-1} x_i, \\
\mu_+ &= \frac{1}{|V_+|}\sum_{y_i=+1} x_i,
\end{align}
where $V_- = \{i : y_i = -1\}$ and $V_+ = \{i : y_i = +1\}$.

The unit spine direction is computed as:
\[
w = \frac{\mu_+ - \mu_-}{\|\mu_+ - \mu_-\|}.
\]

For each data point $x_i$, we compute its projection onto the line passing through $\mu_-$ in direction $w$:
\begin{align}
x_i' &= \mu_- + \langle x_i-\mu_-,\, w\rangle\, w, \\
t_i &= \langle x_i-\mu_-,\, w\rangle.
\end{align}

The projected points $\{x_i'\}$ are then sorted by their projection coordinates $t_i$.

\subsubsection{Augmented Tree Construction}

We construct an augmented tree $T=(V,E)$ with two types of edges:
\begin{enumerate}
\item \textbf{Spoke edges}: Connect each original point $x_i$ to its projection $x_i'$, weighted by the Euclidean distance $\|x_i-x_i'\|$.
\item \textbf{Spine edges}: Connect consecutive projections in the sorted order, weighted by their Euclidean distance in $\mathbb{R}^d$.
\end{enumerate}

For any vertices $u,v \in V$, let $d(u,v)$ denote the shortest path distance (sum of edge weights) between $u$ and $v$ in $T$.

\subsubsection{Objective Function}

For two vertices $u,v$ with $y(u) \neq y(v)$, we define the noise terms:
\begin{align}
f_+(u,v) &= \sum_{z\in V_+(u,v)} d(z,u), \\
f_-(v,u) &= \sum_{z\in V_-(v,u)} d(z,v),
\end{align}
where $V_+(u,v)$ (respectively $V_-(v,u)$) is the set of positive (respectively negative) labeled vertices in the subtree on the branch from $u$ toward $v$ (respectively from $v$ toward $u$) when the tree is rooted at $u$ (respectively at $v$).

The total noise is
\begin{align}
f(u,v) = f_+(u,v) + f_-(v,u)
\end{align}

\addLE{We define the margin as the distance between two support vectors
\begin{equation*}
    d(u,v).
\end{equation*}
Since we aim to minimize the noise while maximizing the margin, the total loss function is defined as
\begin{equation*}
    L_\lambda(u,v) = f(u,v) - \lambda \cdot d(u,v).
\end{equation*}
where $\lambda \geq 0$ is a parameter which controls the balance between the noise and the margin.
}


In this work, we focus on the case $\lambda=1$ and write $L(u,v) = f(u,v) - d(u,v)$. The decision boundary corresponding to a support pair $(s,p)$ is the perpendicular bisector hyperplane of the segment $sp$ in $\mathbb{R}^d$.

\subsection{Theoretical Properties}

In this subsection, we establish the key theoretical properties of our SVM on tree formulation, focusing on the case $\lambda=1$.

\begin{figure}[htbp]
  \centering
  \begin{tikzpicture}[scale=1.5, thick]
    % Vertices
    \node[draw, circle, fill=black, inner sep=2pt, label=above:$p_4$] (P4) at (0,0) {};
    \node[draw, circle, fill=black, inner sep=2pt, label=above:$s$] (S) at (2,0) {};
    \node[draw, circle, fill=black, inner sep=2pt, label=above:$p_2$] (P2) at (4,0) {};
    \node[draw, circle, fill=black, inner sep=2pt, label=above:$p_1$] (P1) at (6,0) {};
    \node[draw, circle, fill=black, inner sep=2pt, label=below:$p_3$] (P3) at (4,-1.5) {};

    % Edges with labels
    \draw (P4) -- node[above] {$z$} (S);
    \draw (S) -- node[above] {$x$} (P2);
    \draw (P2) -- node[above] {$y$} (P1);
    \draw (P2) -- (P3);
  \end{tikzpicture}
  \caption{Tree structure illustrating nodes $s$, $p_1$, $p_2$, $p_3$, and $p_4$ with edge weights $x$, $y$, and $z$.}
  \label{fig:tree_nodes}
\end{figure}


\subsubsection{Adjacency Property}

We now establish a fundamental property that significantly reduces the search space for optimal support pairs.


Let $s \in V_+$ and $p_1, p_2\in V_-$ and $p_2$ is in the path connecting $s$ and $p_1$, \ie
\begin{equation}
    d(s, p_1) = d(s, p_2) + d(p_2, p_1)
    \label{eq:distance-decomposition}
\end{equation}
Define the residual of vertex set 
\begin{equation*}
    R = V_-(p_1, s) \setminus V_-(p_2, s).
\end{equation*}
The following result establish the difference between the two noise functions.
\begin{lemma}
    We have 
    \begin{equation*}
    f(s, p_1) - f(s, p_2) = d(p_2, p_1) |V_-(p_2,s)| + \sum_{v\in R} d(v, p_1).
\end{equation*}
\end{lemma}
\begin{proof}
    Recall the definition of total noise, see XXX, we have 
    \begin{align*}
        f(s, p_1) & = f_+(s, p_1) + f_-(p_1, s)\\ 
        f(s, p_2) & = f_+(s, p_2) + f_-(p_2, s).
    \end{align*}
    Since $f_+(s, p_1) = f_+(s, p_2)$, \remLE{Why? Explain.}, then 
    \begin{equation*}
        f(s, p_1) - f(s, p_2) = f_-(p_1, s) - f_-(p_2, s).
    \end{equation*}

    Recall the definition of negative noise, we have
    \begin{equation*}
        f_-(p_1, s) = \sum_{v\in V_-(p_1, s)} d(v, p_1)
    \end{equation*}



    Then 
    \begin{align*}
        f_-(p_1, s)
        & = \sum_{v\in V_-(p_1, s)} d(v, p_1)\\
        & = \sum_{v\in V_-(p_2, s)} d(v, p_1) + \sum_{v\in R} d(v, p_1)\\
        & = \sum_{v\in V_-(p_2, s)} [d(v, p_2) + d(p_2, p_1)] + \sum_{v\in R} d(v, p_1)\\
        & = f_-(p_2, s) + d(p_2, p_1) |V_-(p_2,s)| + \sum_{v\in R} d(v, p_1)
    \end{align*}
    or 
    \begin{equation*}
        f_-(p_1, s) - f_-(p_2, s) = d(p_2, p_1) |V_-(p_2,s)| + \sum_{v\in R} d(v, p_1).
    \end{equation*}

    
\end{proof}


\begin{theorem}[Adjacency Property]
\label{thm:adjacency}
For $\lambda = 1$, there exists an optimal support pair $(s^*,p^*)$ consisting of two adjacent vertices on the spine.
\end{theorem}

\begin{proof}
    We prove by contradiction.
    Let $s \in V_+$ and $p_1 \in V_-$ and $(s, p_1)$ is optimal but not adjacent, \ie there exists $p_2 \in V_-$ \remLE{why} in the path connecting $s$ and $p_1$. 


    Now, we aim to show that $(s, p_2)$ is also optimal but having $p_2$ closer to $s$ in a comparison with $p_1$.
    To this end, we consider 
    \begin{align*}
        L_1(s, p_1) - L_1(s, p_2) 
        & = f(s, p_1) - f(s, p_2) - (d(s, p_1) -d(s, p_2))\\
        & = d(p_2, p_1) |V_-(p_2,s)| + \sum_{v\in R} d(v, p_1) - d(p_2, p_1)\\
        & = d(p_2, p_1) (|V_-(p_2,s)| - 1) + \sum_{v\in R} d(v, p_1).
    \end{align*}
    Since $V_-(p_2,s)$ contains at least a vertex (say $p_1$), we should have $|V_-(p_2,s)| - 1 \geq 0$. Thus, we obtain the nonnegativity. Therefore, 
    \begin{equation*}
        L_1(s, p_1) \geq L_1(s, p_2).
    \end{equation*}
    Since $(s, p_1)$ is optimal, $(s, p_2)$ is also optimal. This completes the proof.

    Note that, there exists an optimal pair $(s^*, p^*)$ lying on the spine. Otherwise, assume that $p^*$ not on the spine, \ie it is the original point, let $p'$ be its projection, then it is clear that $p'$ has the same sign as $p^*$ while closer to $s$. Thus $(s^*, p')$ is also optimal with $p'$ on spine. If $s^*$ is not on the spine, we repeat the same argument. We complete the proof.
\end{proof}


\subsection{Algorithm and Computational Complexity}

\subsubsection{Preprocessing} 

Recall $x'_1, ...,x'_n$ are projections of $x_1,..., x_n$ and we assume that $x'_1, ...,x'_n$ have been sorted in a certain order, \eg by their first coordinates. By theorem XXX, we know that support vectors is a pair $(x'_i, x'_{i+1})$ with opposite signs. The margin is clearly $d(x'_i, x'_{i+1}) = \|x'_i - x'_{i+1}\|_2$.
To compute the total loss, we only need $f(x'_i, x'_{i+1})$ for $i=1,.., n-1$. In the following, we will clarify how to compute these values efficiently using dynamic programming.


For $i = 1, ...., n-1$, we define 
\begin{align*}
    N^{>i}_+  & = |\{v \in V_+(x'_i, x'_{i+1})\}| = 2 |\{j > i: y_j=+1\}|\\
    N^{>i}_-  & = |\{v \in V_-(x'_i, x'_{i+1})\}| = 2 |\{j > i: y_j=-1\}|\\
    N^{\leq i}_+  & = |\{v \in V_+(x'_{i+1}, x'_{i})\}| = 2 |\{j \leq  i: y_j=+1\}|\\
    N^{\leq i}_-  & = |\{v \in V_+(x'_{i+1}, x'_{i})\}| = 2 |\{j \leq  i: y_j=-1\}|
\end{align*}
and
\begin{align*}
    d^{>i}_+  & = \sum_{v \in V_+(x'_i, x'_{i+1})} d(x'_i, v)
    & = \sum_{j > i, y_j = +1 }[d(x'_i, x'_j) + d(x'_i, x_j)],\\
    d^{>i}_-  & = \sum_{v \in V_-(x'_i, x'_{i+1})} d(x'_i, v)
    & = \sum_{j > i, y_j = -1 }[d(x'_i, x'_j) + d(x'_i, x_j)],\\
    d^{\leq i}_+  & = \sum_{v \in V_+(x'_{i+1}, x'_{i})} d(v, x'_{i+1})
    & = \sum_{j \leq i, y_j = +1 }[d(x'_j, x'_{i+1}) + d(x_j, x'_{i+1})]\\
    d^{\leq i}_-  & = \sum_{v \in V_-(x'_{i+1}, x'_{i})} d(v, x'_{i+1})
    & = \sum_{j \leq i, y_j = -1 }[d(x'_j, x'_{i+1}) + d(x_j, x'_{i+1})].
\end{align*}

Consider $i = 1, ...., n-1$. If $y_i = -1$ and $y_{i+1}=+1$, we then have 
\begin{equation*}
    f(x'_i, x'_{i+1}) = d^{>i}_- + d^{\leq i}_+.
\end{equation*}
Else, that is $y_i = +1$ and $y_{i+1}=-1$, then 
\begin{equation*}
    f(x'_i, x'_{i+1}) = d^{>i}_+ + d^{\leq i}_-.
\end{equation*}

\begin{proposition}
    For $i=1,..., n-1$, if we have $N^{\leq i}_-$ and $d^{\leq i}_-$, then we can update $N^{\leq i+1}_-$ and $d^{\leq i+1}_-$.
\end{proposition}
\begin{proof}
    By definition,
    \[
        d^{\leq i}_- = \sum_{\substack{j \le i \\ y_j=-1}} \big( d(x'_j, x'_{i+1}) + d(x_j, x'_{i+1}) \big),\qquad
        d^{\leq i+1}_- = \sum_{\substack{j \le i+1 \\ y_j=-1}} \big( d(x'_j, x'_{i+2}) + d(x_j, x'_{i+2}) \big).
    \]
    First consider the sum over indices $j \le i$ with $y_j=-1$. Due to the tree structure consisting of the spine (connecting the $x'_k$ in order) and spokes (connecting each $x_k$ to its projection $x'_k$), for all $j \le i$, the unique path from $x'_j$ (or $x_j$) to $x'_{i+2}$ passes through $x'_{i+1}$. Therefore,
    \[
        d(x'_j, x'_{i+2}) = d(x'_j, x'_{i+1}) + d(x'_{i+1}, x'_{i+2}),\qquad
        d(x_j,  x'_{i+2}) = d(x_j,  x'_{i+1}) + d(x'_{i+1}, x'_{i+2}).
    \]
    Summing over all $j \le i$ with $y_j=-1$, the increase due to shifting the anchor from $x'_{i+1}$ to $x'_{i+2}$ is
    \[
        N^{\le i}_- \cdot d(x'_{i+1}, x'_{i+2}),
    \]
    since $N^{\le i}_-$ counts exactly the number of negative vertices participating in the sum (each index $j$ contributes two vertices $x'_j$ and $x_j$).

    Additionally, if $y_{i+1}=-1$, two new vertices appear in the region $\le i+1$, namely $x'_{i+1}$ and $x_{i+1}$. Their contribution to $d^{\le i+1}_-$ is
    \[
        d(x'_{i+1}, x'_{i+2}) + d(x_{i+1}, x'_{i+2})
        = d(x'_{i+1}, x'_{i+2}) + \big( d(x_{i+1}, x'_{i+1}) + d(x'_{i+1}, x'_{i+2}) \big)
        = 2\,d(x'_{i+1}, x'_{i+2}) + d(x_{i+1}, x'_{i+1}).
    \]
    Combining both parts, we obtain the update formulas for the two cases based on the label $y_{i+1}$:
    \begin{align*}
        N^{\le i+1}_- &= \begin{cases}
            N^{\le i}_- + 2, & \text{if } y_{i+1}=-1,\\
            N^{\le i}_-, & \text{if } y_{i+1}=+1,
        \end{cases}\\[4pt]
        d^{\le i+1}_- &= \begin{cases}
            d^{\le i}_- + N^{\le i}_-\, d(x'_{i+1}, x'_{i+2}) + 2\,d(x'_{i+1}, x'_{i+2}) + d(x_{i+1}, x'_{i+1}), & \text{if } y_{i+1}=-1,\\
            d^{\le i}_- + N^{\le i}_-\, d(x'_{i+1}, x'_{i+2}), & \text{if } y_{i+1}=+1.
        \end{cases}
    \end{align*}
    Clearly, each update step only uses quantities known at step $i$ and distances between consecutive spine points, so it can be performed in constant time. Therefore, knowing $N^{\le i}_-$ and $d^{\le i}_-$ is sufficient to update $N^{\le i+1}_-$ and $d^{\le i+1}_-$.
\end{proof}

By above proposition, we have
\begin{proposition}
    We can compute the following quantities in $O(n)$.
    \begin{enumerate}
        \item $(N^{>i}_+, N^{>i}_-, N^{\leq i}_+, N^{\leq i}_-)$ for all $i=1,..., n-1$
        \item $(d^{>i}_+, d^{>i}_-, d^{\leq i}_+, d^{\leq i}_-)$ for all $i=1,..., n-1$
        \item $f(x'_i, x'_{i+1})$ for all $i=1,..., n-1$.
    \end{enumerate}
\end{proposition}
\begin{proof}
\begin{proof}
    We prove this by applying the previous proposition in a systematic manner, using the dynamic programming technique on trees (Proposition 5.3).

    \textbf{Step 1: Computing all $N$ quantities in $O(n)$.}
    The counting quantities can be computed by simple linear scans:
    \begin{itemize}
        \item $N^{>i}_+, N^{>i}_-$: Scan from right to left, counting positive/negative labels.
        \item $N^{\leq i}_+, N^{\leq i}_-$: Scan from left to right, counting positive/negative labels.
    \end{itemize}
    Each quantity is updated in $O(1)$ per position, giving total $O(n)$ time.

    \textbf{Step 2: Computing all $d$ quantities using dynamic programming on trees.}
    This is where we apply Proposition 5.3 (dynamic programming on trees). The key insight is that our augmented tree has a spine structure, which allows efficient re-rooting operations.

    \textbf{Left-to-right pass} (computing $d^{\leq i}_+, d^{\leq i}_-$):
    \begin{itemize}
        \item Initialize: $d^{\leq 1}_+ = 2d(x'_1, x'_2) + d(x_1, x'_2)$ if $y_1 = +1$, else $0$.
        \item For $i = 1, 2, \ldots, n-2$: Apply Proposition 1 (and its symmetric version for positive labels) to update $d^{\leq i+1}_\pm$ from $d^{\leq i}_\pm$ in $O(1)$ time.
    \end{itemize}

    \textbf{Right-to-left pass} (computing $d^{>i}_+, d^{>i}_-$):
    \begin{itemize}
        \item Initialize: $d^{>n-1}_+ = 0, d^{>n-1}_- = 0$ (no points beyond position $n-1$).
        \item For $i = n-1, n-2, \ldots, 2$: Use the symmetric update formula to compute $d^{>(i-1)}_\pm$ from $d^{>i}_\pm$ in $O(1)$ time.
    \end{itemize}

    The symmetric update formula for the right-to-left pass is obtained by reversing the roles: when moving from anchor $x'_{i+1}$ to $x'_i$, all points $j > i$ have their distances increased by $d(x'_i, x'_{i+1})$, and if $y_i = -1$, we add the contribution of the new point $i$.

    \textbf{Step 3: Computing $f(x'_i, x'_{i+1})$ in $O(n)$.}
    Once all $N$ and $d$ quantities are computed, we can evaluate $f(x'_i, x'_{i+1})$ for each $i = 1, \ldots, n-1$ using the given formulas:
    \begin{itemize}
        \item If $y_i = -1, y_{i+1} = +1$: $f(x'_i, x'_{i+1}) = d^{>i}_- + d^{\leq i}_+$.
        \item If $y_i = +1, y_{i+1} = -1$: $f(x'_i, x'_{i+1}) = d^{>i}_+ + d^{\leq i}_-$.
    \end{itemize}
    Each evaluation takes $O(1)$ time, giving total $O(n)$ time.

    \textbf{Conclusion:} By Proposition 5.3 (dynamic programming on trees), the re-rooting operations along the spine can be performed efficiently. Since each of the three steps requires $O(n)$ time, the total time complexity is $O(n)$.
\end{proof}
\end{proof}



\subsubsection{Dynamic Programming for Objective Evaluation}

To efficiently compute $f(u,v)$ for any pair $(u,v)$, we employ a dynamic programming approach using two depth-first search (DFS) passes along the branch $u \to v$:

\begin{enumerate}
\item \textbf{Size computation pass}: Root the tree $T$ at vertex $u$ and compute, for every node $a$, the subtree sizes:
\begin{align}
\mathrm{sz}_+(a) &= |\{z \in \text{subtree}(a) : y(z) = +1\}|, \\
\mathrm{sz}_-(a) &= |\{z \in \text{subtree}(a) : y(z) = -1\}|.
\end{align}

\item \textbf{Distance accumulation pass}: Still with the tree rooted at $u$, accumulate the weighted distances:
\begin{align}
\mathrm{dist}_+(a) &= \sum_{\substack{z \in \text{subtree}(a)\\ y(z) = +1}} d(z,a), \\
\mathrm{dist}_-(a) &= \sum_{\substack{z \in \text{subtree}(a)\\ y(z) = -1}} d(z,a).
\end{align}

These values are then combined along the unique path $u \to v$ to obtain $f_+(u,v)$. By symmetry, rooting the tree at $v$ yields $f_-(v,u)$.
\end{enumerate}

Since our augmented tree has a spine-and-spoke structure (essentially a path with leaves), all tree traversals can be performed in linear time.

\subsubsection{Complete Algorithm}

\begin{algorithm}[htbp]
\caption{SVM on Tree Algorithm}
\label{alg:svm_tree}
\begin{algorithmic}[1]
\STATE \textbf{Input:} Labeled dataset $\{(x_i,y_i)\}_{i=1}^n \subset \mathbb{R}^d \times \{-1,+1\}$
\STATE \textbf{Output:} Optimal support pair $(s^*,p^*)$ and decision boundary

\STATE Compute class means $\mu_-$, $\mu_+$ and spine direction $w$
\STATE Project all points: $x_i' = \mu_- + \langle x_i-\mu_-, w\rangle w$ for $i = 1,\ldots,n$
\STATE Sort projections $\{x_i'\}$ by coordinate $t_i = \langle x_i-\mu_-, w\rangle$
\STATE Construct augmented tree $T$ with spoke and spine edges
\STATE Initialize $L_{\min} = \infty$, $(s^*,p^*) = \emptyset$

\FOR{each pair of adjacent spine vertices $(s,p)$ with $y(s) \neq y(p)$}
    \STATE Compute $f(s,p)$ using two DFS passes
    \STATE Calculate $L(s,p) = f(s,p) - d(s,p)$
    \IF{$L(s,p) < L_{\min}$}
        \STATE $L_{\min} = L(s,p)$
        \STATE $(s^*,p^*) = (s,p)$
    \ENDIF
\ENDFOR

\STATE \textbf{return} $(s^*,p^*)$ and perpendicular bisector of segment $s^*p^*$
\end{algorithmic}
\end{algorithm}

\subsubsection{Complexity Analysis}

The computational complexity of our algorithm is analyzed as follows:

\begin{itemize}
\item \textbf{Projection step}: Computing projections for $n$ points in $\mathbb{R}^d$ requires $O(nd)$ operations.
\item \textbf{Sorting step}: Sorting $n$ projections by their coordinates takes $O(n \log n)$ time.
\item \textbf{Tree construction}: Building the augmented tree with $n$ spoke edges and $O(n)$ spine edges requires $O(n)$ time.
\item \textbf{Objective evaluation}: Each DFS pass takes $O(n)$ time, and there are $O(n)$ adjacent pairs to evaluate due to Theorem~\ref{thm:adjacency}.
\end{itemize}

\begin{theorem}[Complexity]
The overall time complexity of Algorithm~\ref{alg:svm_tree} is $O(nd + n \log n)$, or $O(n \log n)$ for fixed dimension $d$.
\end{theorem}

This represents a significant improvement over exhaustive search, which would require $O(n^2)$ pair evaluations, each taking $O(n)$ time for objective computation.

\subsubsection{Optimization Problem Statement}

The core optimization problem solved by our algorithm is:

\begin{problem}[SVM on Tree]
Given the augmented tree $T$ and loss function $L(\cdot,\cdot)$, find:
\[
(s^*,p^*) = \arg\min_{(s,p): y(s) \neq y(p)} L(s,p) = \arg\min_{(s,p): y(s) \neq y(p)} [f(s,p) - d(s,p)].
\]
\end{problem}

By Theorem~\ref{thm:adjacency}, this optimization reduces to considering only adjacent opposite-label projection pairs on the spine, dramatically reducing the search space from $O(n^2)$ to $O(n)$ candidates.
