% \section{SVM on tree}
% \remLE{Huan do this}

% \subsection{Modeling}
% \subsection{MaFor theFor the analysisFor example, referring to Figure~\ref{fig:tree_nodes}:
\[
\mathrm{subtree\_cost}(P_2,P_1) = \mathrm{dist}(P_4,P_2) + \mathrm{dist}(P_3,P_2).
\]

\subsubsection{Adjacency Property}

We now establish a fundamental property that significantly reduces the search space for optimal support pairs.

\begin{theorem}[Adjacency Property]
\label{thm:adjacency}
For $\lambda = 1$, the optimal support pair $(s^*,p^*)$ consists of two adjacent vertices on the spine.
\end{theorem}

\begin{proof}
Consider two candidate support pairs: $(s,P_1)$ and $(s,P_2)$ where $P_1$ and $P_2$ are consecutive spine vertices as shown in Figure~\ref{fig:tree_nodes}.he following notation:
\begin{align}
\mathrm{dist}(x,y) &: \text{distance between nodes } x \text{ and } y \text{ in the tree}, \\
\mathrm{subtree\_cost}(x,y) &: \text{sum of distances from } x \text{ to all descendants} \nonumber \\
&\quad \text{with the same label as } x, \text{ when rooted at } y.
\end{align}

For example, referring to Figure~\ref{fig:tree_nodes}:, we introduce the following notation:
\begin{align}
\mathrm{dist}(x,y) &: \text{distance between nodes } x \text{ and } y \text{ in the tree}, \\
\mathrm{subtree\_cost}(x,y) &: \text{sum of distances from } x \text{ to all descendants} \nonumber \\
&\quad \text{with the same label as } x, \text{ when rooted at } y.
\end{align}

For example, referring to Figure~\ref{fig:tree_nodes}:ties}
% \subsection{Dynamic programming}

\section{SVM on Tree}

In this section, we present a novel approach to binary classification using Support Vector Machines (SVM) on tree structures. Our method constructs an augmented tree from the data and formulates the classification problem as an optimization over support pairs on this tree.

\subsection{Problem Formulation}

\subsubsection{Data Representation and Spine Construction}

Given a labeled dataset $\{(x_i,y_i)\}_{i=1}^n \subset \mathbb{R}^d \times \{-1,+1\}$, we define the class means as:
\begin{align}
\mu_- &= \frac{1}{|V_-|}\sum_{y_i=-1} x_i, \\
\mu_+ &= \frac{1}{|V_+|}\sum_{y_i=+1} x_i,
\end{align}
where $V_- = \{i : y_i = -1\}$ and $V_+ = \{i : y_i = +1\}$.

The unit spine direction is computed as:
\[
w = \frac{\mu_+ - \mu_-}{\|\mu_+ - \mu_-\|}.
\]

For each data point $x_i$, we compute its projection onto the line passing through $\mu_-$ in direction $w$:
\begin{align}
x_i' &= \mu_- + \langle x_i-\mu_-,\, w\rangle\, w, \\
t_i &= \langle x_i-\mu_-,\, w\rangle.
\end{align}

The projected points $\{x_i'\}$ are then sorted by their projection coordinates $t_i$.

\subsubsection{Augmented Tree Construction}

We construct an augmented tree $T=(V,E)$ with two types of edges:
\begin{enumerate}
\item \textbf{Spoke edges}: Connect each original point $x_i$ to its projection $x_i'$, weighted by the Euclidean distance $\|x_i-x_i'\|$.
\item \textbf{Spine edges}: Connect consecutive projections in the sorted order, weighted by their Euclidean distance in $\mathbb{R}^d$.
\end{enumerate}

For any vertices $u,v \in V$, let $d(u,v)$ denote the shortest path distance (sum of edge weights) between $u$ and $v$ in $T$.

\subsubsection{Objective Function}

For two vertices $u,v$ with $y(u) \neq y(v)$, we define the noise terms:
\begin{align}
f_+(u,v) &= \sum_{z\in V_+(u,v)} d(z,u), \\
f_-(v,u) &= \sum_{z\in V_-(v,u)} d(z,v),
\end{align}
where $V_+(u,v)$ (respectively $V_-(v,u)$) is the set of positive (respectively negative) labeled vertices in the subtree on the branch from $u$ toward $v$ (respectively from $v$ toward $u$) when the tree is rooted at $u$ (respectively at $v$).

The total noise and loss function are defined as:
\begin{align}
f(u,v) &= f_+(u,v) + f_-(v,u), \\
L_\lambda(u,v) &= f(u,v) - \lambda \cdot d(u,v).
\end{align}

In this work, we focus on the case $\lambda=1$ and write $L(u,v) = f(u,v) - d(u,v)$. The decision boundary corresponding to a support pair $(s,p)$ is the perpendicular bisector hyperplane of the segment $sp$ in $\mathbb{R}^d$.

\subsection{Theoretical Properties}

In this subsection, we establish the key theoretical properties of our SVM on tree formulation, focusing on the case $\lambda=1$.

\begin{figure}[htbp]
  \centering
  \begin{tikzpicture}[scale=1.5, thick]
    % Vertices
    \node[draw, circle, fill=black, inner sep=2pt, label=above:$P_4$] (P4) at (0,0) {};
    \node[draw, circle, fill=black, inner sep=2pt, label=above:$s$] (S) at (2,0) {};
    \node[draw, circle, fill=black, inner sep=2pt, label=above:$P_2$] (P2) at (4,0) {};
    \node[draw, circle, fill=black, inner sep=2pt, label=above:$P_1$] (P1) at (6,0) {};
    \node[draw, circle, fill=black, inner sep=2pt, label=below:$P_3$] (P3) at (4,-1.5) {};

    % Edges with labels
    \draw (P4) -- node[above] {$z$} (S);
    \draw (S) -- node[above] {$x$} (P2);
    \draw (P2) -- node[above] {$y$} (P1);
    \draw (P2) -- (P3);
  \end{tikzpicture}
  \caption{Tree structure illustrating nodes $s$, $P_1$, $P_2$, $P_3$, and $P_4$ with edge weights $x$, $y$, and $z$.}
  \label{fig:tree_nodes}
\end{figure}

\subsubsection{Notation and Definitions}

For the analysis, we introduce the following notation:
\begin{align}
\mathrm{dist}(x,y) &: \text{distance between nodes } x \text{ and } y \text{ in the tree}, \\
\mathrm{subtree\_cost}(x,y) &: \text{sum of distances from } x \text{ to all descendants} \nonumber \\
&\quad \text{with the same label as } x, \text{ when rooted at } y.
\end{align}

For example, referring to Figure~\ref{fig:tree_nodes}:
\[
\mathrm{subtree\_cost}(P_2,P_1) = \mathrm{dist}(P_4,P_2) + \mathrm{dist}(P_3,P_2).
\]

\subsubsection{Adjacency Property}

We now establish a fundamental property that significantly reduces the search space for optimal support pairs.

\begin{theorem}[Adjacency Property]
\label{thm:adjacency}
For $\lambda = 1$, the optimal support pair $(s^*,p^*)$ consists of two adjacent vertices on the spine.
\end{theorem}

\begin{proof}
Consider two candidate support pairs: $(s,P_1)$ and $(s,P_2)$ where $P_1$ and $P_2$ are consecutive spine vertices as shown in Figure~\ref{fig:tree_nodes}.

From the tree structure, we can express:
\begin{align}
f(s,P_1) &= \mathrm{subtree\_cost}(P_2,P_1) + n'(P_1,s) \cdot y, 
f(s,P_2) &= \mathrm{subtree\_cost}(P_2,P_1) - \mathrm{dist}(P_2,P_3),
\end{align}
where $n'(P_1,s)$ represents the number of vertices with the same label as $P_1$ in the relevant subtree.

Therefore:
\[
f(s,P_1) = f(s,P_2) + n'(P_1,s) \cdot y + \mathrm{dist}(P_2,P_3).
\]

Since $\mathrm{dist}(s,P_1) = \mathrm{dist}(s,P_2) + y$, the loss functions are:
\begin{align}
L(s,P_1) &= f(s,P_1) - \mathrm{dist}(s,P_1), 
L(s,P_2) &= f(s,P_2) - \mathrm{dist}(s,P_2).
\end{align}

Substituting the expressions:
\[
L(s,P_1) = L(s,P_2) + n'(P_1,s) \cdot y + \mathrm{dist}(P_2,P_3) - y.
\]

Since $n'(P_1,s) \cdot y \geq 0$, $\mathrm{dist}(P_2,P_3) \geq 0$, and typically $n'(P_1,s) \geq 1$, we have:
\[
L(s,P_1) \geq L(s,P_2).
\]

This implies that among non-adjacent pairs, there always exists an adjacent pair with better or equal loss, establishing the adjacency property.
\end{proof}

\subsection{Detailed derivation of $f(s,p_1)$ and $f(s,p_2)$}

From the construction of the tree we recall that:
\begin{itemize}
    \item $dis(x,y)$ denotes the distance between two nodes $x$ and $y$.
    \item $tol(x,y)$ denotes the sum of distances from $x$ to all descendant nodes with the same label as $x$, when the tree is rooted at $y$.
\end{itemize}

\paragraph{Computation of $f(s,p_1)$.}
Consider the path from $s$ to $p_1$. By definition,
\[
f(s,p_1) = tol(p_2,p_1) + n'(p_1,s)\cdot y,
\]
where the second term counts the contribution of the descendants along the branch from $p_1$ back to $s$.

\paragraph{Computation of $f(s,p_2)$.}
Similarly, consider the path from $s$ to $p_2$. We obtain
\[
f(s,p_2) = tol(p_4,p_1) + n'(p_2,s)\cdot (x+z).
\]
Since the contribution of $p_3$ must be subtracted, this simplifies to
\[
f(s,p_2) = tol(p_2,p_1) - dis(p_2,p_3).
\]

\paragraph{Relation between $f(s,p_1)$ and $f(s,p_2)$.}
Combining the above,
\[
f(s,p_1) = f(s,p_2) + n'(p_1,s)\cdot y + dis(p_2,p_3).
\]

Thus, the loss terms $L(s,p_1)$ and $L(s,p_2)$ can later be compared directly by expanding with these expressions.

We have
\begin{align}
f(s,p_1) &= tol(p_2,p_1) + n'(p_1,s)\cdot y, \\
f(s,p_2) &= tol(p_4,p_1) + n'(p_2,s)\cdot (x+z) \\
         &= tol(p_2,p_1) - dis(p_2,p_3).
\end{align}

Hence,
\[
f(s,P_1) = f(s,P_2) + n'(P_1,s)\cdot y + dis(P_2,P_3).
\]

Also,
\[
dis(s,P_1) = dis(s,P_2) + y.
\]

Therefore,
\begin{align}
L_1 &= L(s,P_1) = f(s,P_1) - dis(s,P_1), \\
L_2 &= L(s,P_2) = f(s,P_2) - dis(s,P_2).
\end{align}

Substituting gives
\begin{align}
L_1 &= f(s,P_2) - dis(s,P_2) + n'(P_1,s)\cdot y + dis(P_2,P_3) + y, \\
L_2 &= f(s,P_2) - dis(s,P_2).
\end{align}

Thus,
\[
L_1 = L_2 + n'(P_1,s)\cdot y + dis(P_2,P_3) + y.
\]

Since $n'(P_1,s)\cdot y \ge 0$, $dis(P_2,P_3) \ge 0$, and $y > 0$, it follows that
\[
L_1 > L_2.
\]

\paragraph{Conclusion.}  
The optimal support pair $(s,p)$ must consist of two \emph{adjacent} vertices on the spine.

\subsection{Algorithm and Computational Complexity}

\subsubsection{Dynamic Programming for Objective Evaluation}

To efficiently compute $f(u,v)$ for any pair $(u,v)$, we employ a dynamic programming approach using two depth-first search (DFS) passes along the branch $u \to v$:

\begin{enumerate}
\item \textbf{Size computation pass}: Root the tree $T$ at vertex $u$ and compute, for every node $a$, the subtree sizes:
\begin{align}
\mathrm{sz}_+(a) &= |\{z \in \text{subtree}(a) : y(z) = +1\}|, \\
\mathrm{sz}_-(a) &= |\{z \in \text{subtree}(a) : y(z) = -1\}|.
\end{align}

\item \textbf{Distance accumulation pass}: Still with the tree rooted at $u$, accumulate the weighted distances:
\begin{align}
\mathrm{dist}_+(a) &= \sum_{\substack{z \in \text{subtree}(a)\\ y(z) = +1}} d(z,a), \\
\mathrm{dist}_-(a) &= \sum_{\substack{z \in \text{subtree}(a)\\ y(z) = -1}} d(z,a).
\end{align}

These values are then combined along the unique path $u \to v$ to obtain $f_+(u,v)$. By symmetry, rooting the tree at $v$ yields $f_-(v,u)$.
\end{enumerate}

Since our augmented tree has a spine-and-spoke structure (essentially a path with leaves), all tree traversals can be performed in linear time.

\subsubsection{Complete Algorithm}

\begin{algorithm}[htbp]
\caption{SVM on Tree Algorithm}
\label{alg:svm_tree}
\begin{algorithmic}[1]
\State \textbf{Input:} Labeled dataset $\{(x_i,y_i)\}_{i=1}^n \subset \mathbb{R}^d \times \{-1,+1\}$
\State \textbf{Output:} Optimal support pair $(s^*,p^*)$ and decision boundary

\State Compute class means $\mu_-$, $\mu_+$ and spine direction $w$
\State Project all points: $x_i' = \mu_- + \langle x_i-\mu_-, w\rangle w$ for $i = 1,\ldots,n$
\State Sort projections $\{x_i'\}$ by coordinate $t_i = \langle x_i-\mu_-, w\rangle$
\State Construct augmented tree $T$ with spoke and spine edges
\State Initialize $L_{\min} = \infty$, $(s^*,p^*) = \emptyset$

\For{each pair of adjacent spine vertices $(s,p)$ with $y(s) \neq y(p)$}
    \State Compute $f(s,p)$ using two DFS passes
    \State Calculate $L(s,p) = f(s,p) - d(s,p)$
    \If{$L(s,p) < L_{\min}$}
        \State $L_{\min} = L(s,p)$
        \State $(s^*,p^*) = (s,p)$
    \EndIf
\EndFor

\State \textbf{return} $(s^*,p^*)$ and perpendicular bisector of segment $s^*p^*$
\end{algorithmic}
\end{algorithm}

\subsubsection{Complexity Analysis}

The computational complexity of our algorithm is analyzed as follows:

\begin{itemize}
\item \textbf{Projection step}: Computing projections for $n$ points in $\mathbb{R}^d$ requires $O(nd)$ operations.
\item \textbf{Sorting step}: Sorting $n$ projections by their coordinates takes $O(n \log n)$ time.
\item \textbf{Tree construction}: Building the augmented tree with $n$ spoke edges and $O(n)$ spine edges requires $O(n)$ time.
\item \textbf{Objective evaluation}: Each DFS pass takes $O(n)$ time, and there are $O(n)$ adjacent pairs to evaluate due to Theorem~\ref{thm:adjacency}.
\end{itemize}

\begin{theorem}[Complexity]
The overall time complexity of Algorithm~\ref{alg:svm_tree} is $O(nd + n \log n)$, or $O(n \log n)$ for fixed dimension $d$.
\end{theorem}

This represents a significant improvement over exhaustive search, which would require $O(n^2)$ pair evaluations, each taking $O(n)$ time for objective computation.

\subsubsection{Optimization Problem Statement}

The core optimization problem solved by our algorithm is:

\begin{problem}[SVM on Tree]
Given the augmented tree $T$ and loss function $L(\cdot,\cdot)$, find:
\[
(s^*,p^*) = \arg\min_{(s,p): y(s) \neq y(p)} L(s,p) = \arg\min_{(s,p): y(s) \neq y(p)} [f(s,p) - d(s,p)].
\]
\end{problem}

By Theorem~\ref{thm:adjacency}, this optimization reduces to considering only adjacent opposite-label projection pairs on the spine, dramatically reducing the search space from $O(n^2)$ to $O(n)$ candidates.
