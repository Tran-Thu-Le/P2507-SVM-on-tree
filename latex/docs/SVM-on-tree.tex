\section{SVM on Tree}
\label{sec:svm_on_tree}
\label{sec:svm_tree}

In this section, we present a novel approach to binary classification using Support Vector Machines (SVM) on tree structures. Our method constructs an augmented tree from the data and formulates the classification problem as an optimization over support pairs on this tree.

\subsection{Problem Formulation}

\subsubsection{Data Representation and Spine Construction}

Given a labeled dataset $\{(x_i,y_i)\}_{i=1}^n \subset \mathbb{R}^d \times \{-1,+1\}$, we define the class means as:
\begin{align}
\mu_- &= \frac{1}{|V_-|}\sum_{y_i=-1} x_i, \\
\mu_+ &= \frac{1}{|V_+|}\sum_{y_i=+1} x_i,
\end{align}
where $V_- = \{i : y_i = -1\}$ and $V_+ = \{i : y_i = +1\}$.

The unit spine direction is computed as:
\[
w = \frac{\mu_+ - \mu_-}{\|\mu_+ - \mu_-\|}.
\]

For each data point $x_i$, we compute its projection onto the line passing through $\mu_-$ in direction $w$:
\begin{align}
x_i' &= \mu_- + \langle x_i-\mu_-,\, w\rangle\, w, \\
t_i &= \langle x_i-\mu_-,\, w\rangle.
\end{align}

The projected points $\{x_i'\}$ are then sorted by their projection coordinates $t_i$.

\subsubsection{Augmented Tree Construction}

We construct an augmented tree $T=(V,E)$ with two types of edges:
\begin{enumerate}
\item \textbf{Spoke edges}: Connect each original point $x_i$ to its projection $x_i'$, weighted by the Euclidean distance $\|x_i-x_i'\|$.
\item \textbf{Spine edges}: Connect consecutive projections in the sorted order, weighted by their Euclidean distance in $\mathbb{R}^d$.
\end{enumerate}

For any vertices $u,v \in V$, let $d(u,v)$ denote the shortest path distance (sum of edge weights) between $u$ and $v$ in $T$.

\subsubsection{Objective Function}

For two vertices $u,v$ with $y(u) \neq y(v)$, we define the noise terms:
\begin{align}
f_+(u,v) &= \sum_{z\in V_+(u,v)} d(z,u), \\
f_-(v,u) &= \sum_{z\in V_-(v,u)} d(z,v),
\end{align}
where $V_+(u,v)$ (respectively $V_-(v,u)$) is the set of positive (respectively negative) labeled vertices in the subtree on the branch from $u$ toward $v$ (respectively from $v$ toward $u$) when the tree is rooted at $u$ (respectively at $v$).

The total noise is
\begin{align}
f(u,v) = f_+(u,v) + f_-(v,u)
\end{align}

\addLE{We define the margin as the distance between two support vectors
\begin{equation*}
    d(u,v).
\end{equation*}
Since we aim to minimize the noise while maximizing the margin, the total loss function is defined as
\begin{equation*}
    L_\lambda(u,v) = f(u,v) - \lambda \cdot d(u,v).
\end{equation*}
where $\lambda \geq 0$ is a parameter which controls the balance between the noise and the margin.
}


In this work, we focus on the case $\lambda=1$ and write $L(u,v) = f(u,v) - d(u,v)$. The decision boundary corresponding to a support pair $(s,p)$ is the perpendicular bisector hyperplane of the segment $sp$ in $\mathbb{R}^d$.

\subsection{Theoretical Properties}

In this subsection, we establish the key theoretical properties of our SVM on tree formulation, focusing on the case $\lambda=1$.

\begin{figure}[htbp]
  \centering
  \begin{tikzpicture}[scale=1.5, thick]
    % Vertices
    \node[draw, circle, fill=black, inner sep=2pt, label=above:$p_4$] (P4) at (0,0) {};
    \node[draw, circle, fill=black, inner sep=2pt, label=above:$s$] (S) at (2,0) {};
    \node[draw, circle, fill=black, inner sep=2pt, label=above:$p_2$] (P2) at (4,0) {};
    \node[draw, circle, fill=black, inner sep=2pt, label=above:$p_1$] (P1) at (6,0) {};
    \node[draw, circle, fill=black, inner sep=2pt, label=below:$p_3$] (P3) at (4,-1.5) {};

    % Edges with labels
    \draw (P4) -- node[above] {$z$} (S);
    \draw (S) -- node[above] {$x$} (P2);
    \draw (P2) -- node[above] {$y$} (P1);
    \draw (P2) -- (P3);
  \end{tikzpicture}
  \caption{Tree structure illustrating nodes $s$, $p_1$, $p_2$, $p_3$, and $p_4$ with edge weights $x$, $y$, and $z$.}
  \label{fig:tree_nodes}
\end{figure}


\subsubsection{Adjacency Property}

We now establish a fundamental property that significantly reduces the search space for optimal support pairs.


Let $s \in V_+$ and $p_1, p_2\in V_-$ and $p_2$ is in the path connecting $s$ and $p_1$, \ie
\begin{equation}
    d(s, p_1) = d(s, p_2) + d(p_2, p_1)
    \label{eq:distance-decomposition}
\end{equation}
Define the residual of vertex set 
\begin{equation*}
    R = V_-(p_1, s) \setminus V_-(p_2, s).
\end{equation*}

\begin{figure}[h]
  \centering
  \includegraphics[width=0.7\linewidth]{../data/images/diff-set.png}
  \caption{...}
  \label{fig:diff-set}
\end{figure}

The following result establish the difference between the two noise functions.
\begin{lemma}[Difference between noise functions]
    We have 
    \begin{equation*}
    f(s, p_1) - f(s, p_2) = d(p_2, p_1) |V_-(p_2,s)| + \sum_{v\in R} d(v, p_1).
\end{equation*}
\end{lemma}
\begin{proof}
    Recall the definition of total noise, see XXX, we have 
    \begin{align*}
        f(s, p_1) & = f_+(s, p_1) + f_-(p_1, s)\\ 
        f(s, p_2) & = f_+(s, p_2) + f_-(p_2, s).
    \end{align*}
    Since $f_+(s, p_1) = f_+(s, p_2)$, \remLE{Why? Explain.}, then 
    \begin{equation*}
        f(s, p_1) - f(s, p_2) = f_-(p_1, s) - f_-(p_2, s).
    \end{equation*}

    Recall the definition of negative noise, we have
    \begin{equation*}
        f_-(p_1, s) = \sum_{v\in V_-(p_1, s)} d(v, p_1)
    \end{equation*}



    Then 
    \begin{align*}
        f_-(p_1, s)
        & = \sum_{v\in V_-(p_1, s)} d(v, p_1)\\
        & = \sum_{v\in V_-(p_2, s)} d(v, p_1) + \sum_{v\in R} d(v, p_1)\\
        & = \sum_{v\in V_-(p_2, s)} [d(v, p_2) + d(p_2, p_1)] + \sum_{v\in R} d(v, p_1)\\
        & = f_-(p_2, s) + d(p_2, p_1) |V_-(p_2,s)| + \sum_{v\in R} d(v, p_1)
    \end{align*}
    or 
    \begin{equation*}
        f_-(p_1, s) - f_-(p_2, s) = d(p_2, p_1) |V_-(p_2,s)| + \sum_{v\in R} d(v, p_1).
    \end{equation*}

    
\end{proof}


\begin{theorem}[Adjacency Property]
\label{thm:adjacency}
For $\lambda = 1$, there exists an optimal support pair $(s^*,p^*)$ consisting of two adjacent vertices on the spine.
\end{theorem}

\begin{proof}
    We prove by contradiction.
    Let $s \in V_+$ and $p_1 \in V_-$ and $(s, p_1)$ is optimal but not adjacent, \ie there exists $p_2 \in V_-$ \remLE{why} in the path connecting $s$ and $p_1$. 


    Now, we aim to show that $(s, p_2)$ is also optimal but having $p_2$ closer to $s$ in a comparison with $p_1$.
    To this end, we consider 
    \begin{align*}
        L_1(s, p_1) - L_1(s, p_2) 
        & = f(s, p_1) - f(s, p_2) - (d(s, p_1) -d(s, p_2))\\
        & = d(p_2, p_1) |V_-(p_2,s)| + \sum_{v\in R} d(v, p_1) - d(p_2, p_1)\\
        & = d(p_2, p_1) (|V_-(p_2,s)| - 1) + \sum_{v\in R} d(v, p_1).
    \end{align*}
    Since $V_-(p_2,s)$ contains at least a vertex (say $p_1$), we should have $|V_-(p_2,s)| - 1 \geq 0$. Thus, we obtain the nonnegativity. Therefore, 
    \begin{equation*}
        L_1(s, p_1) \geq L_1(s, p_2).
    \end{equation*}
    Since $(s, p_1)$ is optimal, $(s, p_2)$ is also optimal. This completes the proof.

    Note that, there exists an optimal pair $(s^*, p^*)$ lying on the spine. Otherwise, assume that $p^*$ not on the spine, \ie it is the original point, let $p'$ be its projection, then it is clear that $p'$ has the same sign as $p^*$ while closer to $s$. Thus $(s^*, p')$ is also optimal with $p'$ on spine. If $s^*$ is not on the spine, we repeat the same argument. We complete the proof.
\end{proof}


\subsection{Algorithm and Computational Complexity}

\subsubsection{Preprocessing}

We assume that $x_1,..., x_n$ are sorted in a certain order, \eg by their first coordinate, then by the second one, ... 
We root the tree at $x_1$ and assume without loss of generality that $x_1 \in V_+$.


By Theorem XXX, we know that a pair of suport vector is one of the adjacent pair $(x_i, x_{i+1})$ with different signs. 
In this case the margin is easily to compute $d(x_i, x_{i+1})= \|x_i - x_{i+1}\|$. To compute to total loss function, it is necessary to compute $f_+(x_i, x_{i+1})$ and $f_-(x_i, x_{i+1})$.



\subsubsection{Dynamic Programming for Objective Evaluation}

To efficiently compute $f(u,v)$ for any pair $(u,v)$, we employ a dynamic programming approach using two depth-first search (DFS) passes along the branch $u \to v$:

\begin{enumerate}
\item \textbf{Size computation pass}: Root the tree $T$ at vertex $u$ and compute, for every node $a$, the subtree sizes:
\begin{align}
\mathrm{sz}_+(a) &= |\{z \in \text{subtree}(a) : y(z) = +1\}|, \\
\mathrm{sz}_-(a) &= |\{z \in \text{subtree}(a) : y(z) = -1\}|.
\end{align}

\item \textbf{Distance accumulation pass}: Still with the tree rooted at $u$, accumulate the weighted distances:
\begin{align}
\mathrm{dist}_+(a) &= \sum_{\substack{z \in \text{subtree}(a)\\ y(z) = +1}} d(z,a), \\
\mathrm{dist}_-(a) &= \sum_{\substack{z \in \text{subtree}(a)\\ y(z) = -1}} d(z,a).
\end{align}

These values are then combined along the unique path $u \to v$ to obtain $f_+(u,v)$. By symmetry, rooting the tree at $v$ yields $f_-(v,u)$.
\end{enumerate}

Since our augmented tree has a spine-and-spoke structure (essentially a path with leaves), all tree traversals can be performed in linear time.

\subsubsection{Complete Algorithm}

\begin{algorithm}[htbp]
\caption{SVM on Tree Algorithm}
\label{alg:svm_tree}
\begin{algorithmic}[1]
\STATE \textbf{Input:} Labeled dataset $\{(x_i,y_i)\}_{i=1}^n \subset \mathbb{R}^d \times \{-1,+1\}$
\STATE \textbf{Output:} Optimal support pair $(s^*,p^*)$ and decision boundary

\STATE Compute class means $\mu_-$, $\mu_+$ and spine direction $w$
\STATE Project all points: $x_i' = \mu_- + \langle x_i-\mu_-, w\rangle w$ for $i = 1,\ldots,n$
\STATE Sort projections $\{x_i'\}$ by coordinate $t_i = \langle x_i-\mu_-, w\rangle$
\STATE Construct augmented tree $T$ with spoke and spine edges
\STATE Initialize $L_{\min} = \infty$, $(s^*,p^*) = \emptyset$

\FOR{each pair of adjacent spine vertices $(s,p)$ with $y(s) \neq y(p)$}
    \STATE Compute $f(s,p)$ using two DFS passes
    \STATE Calculate $L(s,p) = f(s,p) - d(s,p)$
    \IF{$L(s,p) < L_{\min}$}
        \STATE $L_{\min} = L(s,p)$
        \STATE $(s^*,p^*) = (s,p)$
    \ENDIF
\ENDFOR

\STATE \textbf{return} $(s^*,p^*)$ and perpendicular bisector of segment $s^*p^*$
\end{algorithmic}
\end{algorithm}

\subsubsection{Complexity Analysis}

The computational complexity of our algorithm is analyzed as follows:

\begin{itemize}
\item \textbf{Projection step}: Computing projections for $n$ points in $\mathbb{R}^d$ requires $O(nd)$ operations.
\item \textbf{Sorting step}: Sorting $n$ projections by their coordinates takes $O(n \log n)$ time.
\item \textbf{Tree construction}: Building the augmented tree with $n$ spoke edges and $O(n)$ spine edges requires $O(n)$ time.
\item \textbf{Objective evaluation}: Each DFS pass takes $O(n)$ time, and there are $O(n)$ adjacent pairs to evaluate due to Theorem~\ref{thm:adjacency}.
\end{itemize}

\begin{theorem}[Complexity]
The overall time complexity of Algorithm~\ref{alg:svm_tree} is $O(nd + n \log n)$, or $O(n \log n)$ for fixed dimension $d$.
\end{theorem}

This represents a significant improvement over exhaustive search, which would require $O(n^2)$ pair evaluations, each taking $O(n)$ time for objective computation.

\subsubsection{Optimization Problem Statement}

The core optimization problem solved by our algorithm is:

\begin{problem}[SVM on Tree]
Given the augmented tree $T$ and loss function $L(\cdot,\cdot)$, find:
\[
(s^*,p^*) = \arg\min_{(s,p): y(s) \neq y(p)} L(s,p) = \arg\min_{(s,p): y(s) \neq y(p)} [f(s,p) - d(s,p)].
\]
\end{problem}

By Theorem~\ref{thm:adjacency}, this optimization reduces to considering only adjacent opposite-label projection pairs on the spine, dramatically reducing the search space from $O(n^2)$ to $O(n)$ candidates.
