\section{SVM on Tree}
\label{sec:svm_on_tree}
\label{sec:svm_tree}

In this section, we present a novel approach to binary classification using SVM on tree structures. We first construct an augmented labeled tree from the training data. Second, we formulate a SVM model on tree. Third, we study the theoretial property of the model. Finally, we leverage the established property to propose combinatorial methods solving the problem efficiently.

\subsection{Tree-based approximation for data points}
\label{sec:tree-based-approximation}
Given a training dataset $\{(x_i,y_i)\in \mathbb{R}^d \times \{-1,+1\}: i=1,..., n\}$, we describe a procedure for constructing a tree that approximates the dataset.

Let $\Delta$ be a line in $\mathbb{R}^d$. In this work, we focus on the line $\Delta$ connecting the two class means. For each $i=1,\ldots,n$, let $x'_i$ denote the orthogonal projection of $x_i$ onto $\Delta$. The tree is then built with vertices consisting of both the original points $x_i$ and their projections $x'_i$. Its edge set includes (i) spine edges, which are the line segments connecting consecutive projection points along $\Delta$, and (ii) spoke edges, which are the line segments connecting each original point $x_i$ to its projection $x'_i$. The details of this construction are described below.


Let $V_- = \{x_i : y_i = -1\}$ and $V_+ = \{x_i : y_i = +1\}$.
The class means is:
\begin{align*}
\mu_- &= \frac{1}{|V_-|}\sum_{y_i=-1} x_i, \\
\mu_+ &= \frac{1}{|V_+|}\sum_{y_i=+1} x_i.
\end{align*}

The unit vector pointing the direction of the line $\Delta$ conencting the two means is
\begin{equation*}
    w = \frac{\mu_+ - \mu_-}{\|\mu_+ - \mu_-\|_2}.
\end{equation*}

For each data point $x_i$, we compute its projection onto the line passing through $\mu_-$ in direction $w$
\begin{align*}
x_i' &= \mu_- + t_i w, \\
t_i &= \langle x_i-\mu_-,\, w\rangle.
\end{align*}

The projected points $\{x_i'\}$ are then sorted by their projection coordinates $t_i$. Without loss of generality, we may assume that 
\begin{equation*}
    t_1 \leq ...\leq t_n.
\end{equation*}


We construct an augmented tree $T=(V,E)$ with vertex set
\begin{equation*}
V = \{x_1, ...., x_n\} \cup \{x'_1,..., x'_n\},
\end{equation*}
and edge set
\begin{equation*}
E = \{(x'_i, x'_{i+1}): i=1,..., n-1\} \cup \{(x_i, x'_i): i=1,..., n\}.
\end{equation*}
We refer to edges connecting two adjacent projection points as \emph{spine edges}, and to edges connecting each original point with its projection as \emph{spoke edges}. The path formed by the sequence of all spine edges is called the \emph{spine} of the tree.

The tree $T$ is endowed with vertex labels and edge lengths as follows. Each $x_i$ and $x'_i$ inherits the original label $y_i$ for all $i=1,..., n$. Each edge is assigned a length equal to the Euclidean distance between its endpoints. For any vertices $u,v \in V$, let $d(u,v)$ denote the distance between $u$ and $v$, defined as the total length of edges along the (unique) path connecting them.

\subsection{SVM model on tree}


From XXX, we know that the formulation of the SVM model strongly relies on the support vectors, which are points with different labels.
Let $T$ be the tree constructed in \Cref{sec:tree-based-approximation}.
We focus on a pair of vertices with different labels, say 
\begin{equation*}
    u \in V_+ \text{ and } v \in V_-,
\end{equation*}
which is the analog of support vectors in the tree context.
We seek such pairs of nodes satisfying two criteria.

First, the margin in the classical SVM is modeled as the distance between two support nodes, say $d(u,v)$.
Second, we introduce the notion of noisy nodes in the tree.
Consider a positive support node $u$, and among the subtrees rooted at $u$, take the branch containing the negative node $v$.
We regard the positive nodes on the same branch as $v$ as positive noisy nodes.
This motivates the definition of the set of positive noisy nodes, denoted by $V_+(u,v)$, as the set of positively labeled vertices in the subtree on the branch rooted at $u$ toward $v$.
Similarly, we define $V_-(v,u)$ as the set of negatively labeled vertices on the branch rooted at $v$ toward $u$, referred to as negative noisy nodes.

We define the positive and negative noise function of $u$ and $v$ as follows:
\begin{align}
f_+(u,v) &= \sum_{z\in V_+(u,v)} d(z,u), \\
f_-(v,u) &= \sum_{z\in V_-(v,u)} d(z,v).
\end{align}
The total noise is
\begin{equation*}
    f(u,v) = f_+(u,v) + f_-(v,u)
\end{equation*}
Note that the order of $(u, v)$ is important in XXX, while not important in XXX.

We define the margin as the distance between two support vectors
\begin{equation*}
    d(u,v).
\end{equation*}
Since we aim to minimize the noise while maximizing the margin, the total loss function is defined as
\begin{equation*}
    L_\lambda(u,v) = f(u,v) - \lambda \cdot d(u,v).
\end{equation*}
where $\lambda \geq 0$ is a parameter which controls the balance between the noise and the margin.


The decision boundary corresponding to a support pair $(s,p)$ is the perpendicular bisector hyperplane of the segment between $s$ and $p$ in $\mathbb{R}^d$. 
\remLE{help me discuss the way to do prediction: Given new data point, describe the procedure to make prediction about its label.}

\subsection{Training SVM model on tree with arbitrary parameter}

By definition of SVM model on tree, the naive training approach for  such a model needs the evaluation of objectives for all pairs of nodes. Since there are $O(n^2)$ nodes and $O(n)$ time for computing the objective, the overall complexity for such naive approach is $O(n^3)$.


\remLE{I predict that we can reduce this complexity}
\begin{theorem}
    For any $\lambda \geq 0$, we can find the support nodes in $O(n^2)$.
\end{theorem}

\remLE{For now, we don't really know how to do it. Since the distance matric on tree can be computed in $O(n^2)$. So, the remaining task is to show that $f(u, v)$ should be computed in $O(n^2)$.}

\subsection{Training SVM model on tree with unit parameter}

% \subsubsection{Adjacency property for unit parameter}



% \begin{figure}[htbp]
%   \centering
%   \begin{tikzpicture}[scale=1.5, thick]
%     % Vertices
%     \node[draw, circle, fill=black, inner sep=2pt, label=above:$p_4$] (P4) at (0,0) {};
%     \node[draw, circle, fill=black, inner sep=2pt, label=above:$s$] (S) at (2,0) {};
%     \node[draw, circle, fill=black, inner sep=2pt, label=above:$p_2$] (P2) at (4,0) {};
%     \node[draw, circle, fill=black, inner sep=2pt, label=above:$p_1$] (P1) at (6,0) {};
%     \node[draw, circle, fill=black, inner sep=2pt, label=below:$p_3$] (P3) at (4,-1.5) {};

%     % Edges with labels
%     \draw (P4) -- node[above] {$z$} (S);
%     \draw (S) -- node[above] {$x$} (P2);
%     \draw (P2) -- node[above] {$y$} (P1);
%     \draw (P2) -- (P3);
%   \end{tikzpicture}
%   \caption{Tree structure illustrating nodes $s$, $p_1$, $p_2$, $p_3$, and $p_4$ with edge weights $x$, $y$, and $z$.}
%   \label{fig:tree_nodes}
% \end{figure}


In this subsection, we establish a key theoretical properties of our SVM on tree formulation, focusing on the case $\lambda=1$. Specifically, we show that the two support nodes are adjacent and lies on the spine of the tree. 
This fundamental property significantly reduces the search space for optimal support pairs.


Let $s \in V_+$ and $p_1, p_2\in V_-$ such that $p_2$ is in the path connecting $s$ and $p_1$, \ie
\begin{equation}
    d(s, p_1) = d(s, p_2) + d(p_2, p_1)
    \label{eq:distance-decomposition}
\end{equation}
Define the residual of vertex set 
\begin{equation*}
    R = V_-(p_1, s) \setminus V_-(p_2, s).
\end{equation*}
Please refer to \Cref{fig:diff-set} \remLE{Em giup anh ve hinh nay nhe.} 

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.5\textwidth]{../data/images/diff-set.png}
    \caption{Difference set $R$ illustration.}
    \label{fig:diff-set}
\end{figure}

The following result establish the difference between the two noise functions.
\begin{lemma}
    We have 
    \begin{equation*}
    f(s, p_1) - f(s, p_2) = d(p_2, p_1) |V_-(p_2,s)| + \sum_{v\in R} d(v, p_1).
\end{equation*}
\end{lemma}
\begin{proof}
    Recall the definition of total noise, see XXX, we have 
    \begin{align*}
        f(s, p_1) & = f_+(s, p_1) + f_-(p_1, s)\\ 
        f(s, p_2) & = f_+(s, p_2) + f_-(p_2, s).
    \end{align*}
    Since $f_+(s, p_1) = f_+(s, p_2)$, \remLE{Why? Explain.}, then 
    \begin{equation*}
        f(s, p_1) - f(s, p_2) = f_-(p_1, s) - f_-(p_2, s).
    \end{equation*}

    Recall the definition of negative noise, we have
    \begin{equation*}
        f_-(p_1, s) = \sum_{v\in V_-(p_1, s)} d(v, p_1)
    \end{equation*}



    Then 
    \begin{align*}
        f_-(p_1, s)
        & = \sum_{v\in V_-(p_1, s)} d(v, p_1)\\
        & = \sum_{v\in V_-(p_2, s)} d(v, p_1) + \sum_{v\in R} d(v, p_1)\\
        & = \sum_{v\in V_-(p_2, s)} [d(v, p_2) + d(p_2, p_1)] + \sum_{v\in R} d(v, p_1)\\
        & = f_-(p_2, s) + d(p_2, p_1) |V_-(p_2,s)| + \sum_{v\in R} d(v, p_1)
    \end{align*}
    or 
    \begin{equation*}
        f_-(p_1, s) - f_-(p_2, s) = d(p_2, p_1) |V_-(p_2,s)| + \sum_{v\in R} d(v, p_1).
    \end{equation*}

    
\end{proof}


\begin{theorem}[Adjacency Property]
\label{thm:adjacency}
For $\lambda = 1$, there exists an optimal support pair $(s^*,p^*)$ consisting of two adjacent vertices on the spine.
\end{theorem}

\begin{proof}
    We prove by contradiction.
    Let $s \in V_+$ and $p_1 \in V_-$ and $(s, p_1)$ is optimal but not adjacent, \ie there exists $p_2 \in V_-$ \remLE{why} in the path connecting $s$ and $p_1$. 


    Now, we aim to show that $(s, p_2)$ is also optimal but having $p_2$ closer to $s$ in a comparison with $p_1$.
    To this end, we consider 
    \begin{align*}
        L_1(s, p_1) - L_1(s, p_2) 
        & = f(s, p_1) - f(s, p_2) - (d(s, p_1) -d(s, p_2))\\
        & = d(p_2, p_1) |V_-(p_2,s)| + \sum_{v\in R} d(v, p_1) - d(p_2, p_1)\\
        & = d(p_2, p_1) (|V_-(p_2,s)| - 1) + \sum_{v\in R} d(v, p_1).
    \end{align*}
    Since $V_-(p_2,s)$ contains at least a vertex (say $p_1$), we should have $|V_-(p_2,s)| - 1 \geq 0$. Thus, we obtain the nonnegativity. Therefore, 
    \begin{equation*}
        L_1(s, p_1) \geq L_1(s, p_2).
    \end{equation*}
    Since $(s, p_1)$ is optimal, $(s, p_2)$ is also optimal. This completes the proof.

    \remLE{Hel me write this in details} Note that, there exists an optimal pair $(s^*, p^*)$ lying on the spine. Otherwise, assume that $p^*$ not on the spine, \ie it is the original point, let $p'$ be its projection, then it is clear that $p'$ has the same sign as $p^*$ while closer to $s$. Thus $(s^*, p')$ is also optimal with $p'$ on spine. If $s^*$ is not on the spine, we repeat the same argument. We complete the proof.
\end{proof}


\subsection{Algorithm and Complexity Analysis}

In this section, we focus on the  design of an efficient algorithm for training SVM model on tree with $\lambda=1$.

\subsubsection{Preprocessing} 

Recall $x'_1, ...,x'_n$ are projections of $x_1,..., x_n$ and we assume that $x'_1, ...,x'_n$ have been sorted in a certain order, \eg by their first coordinates. By theorem XXX, we know that support vectors is a pair $(x'_i, x'_{i+1})$ with opposite signs. The margin is clearly $d(x'_i, x'_{i+1}) = \|x'_i - x'_{i+1}\|_2$.
To compute the total loss, we only need $f(x'_i, x'_{i+1})$ for $i=1,.., n-1$. In the following, we will clarify how to compute these values efficiently using dynamic programming.


For $i = 1, ...., n-1$, we define data at iteration $i$ as
\begin{align*}
    N^{>i}_+  & = |\{v \in V_+(x'_i, x'_{i+1})\}| = 2 |\{j > i: y_j=+1\}|\\
    N^{>i}_-  & = |\{v \in V_-(x'_i, x'_{i+1})\}| = 2 |\{j > i: y_j=-1\}|\\
    N^{\leq i}_+  & = |\{v \in V_+(x'_{i+1}, x'_{i})\}| = 2 |\{j \leq  i: y_j=+1\}|\\
    N^{\leq i}_-  & = |\{v \in V_+(x'_{i+1}, x'_{i})\}| = 2 |\{j \leq  i: y_j=-1\}|
\end{align*}
and
\begin{align*}
    d^{>i}_+  & = \sum_{v \in V_+(x'_i, x'_{i+1})} d(x'_i, v)
    & = \sum_{j > i, y_j = +1 }[d(x'_i, x'_j) + d(x'_i, x_j)],\\
    d^{>i}_-  & = \sum_{v \in V_-(x'_i, x'_{i+1})} d(x'_i, v)
    & = \sum_{j > i, y_j = -1 }[d(x'_i, x'_j) + d(x'_i, x_j)],\\
    d^{\leq i}_+  & = \sum_{v \in V_+(x'_{i+1}, x'_{i})} d(v, x'_{i+1})
    & = \sum_{j \leq i, y_j = +1 }[d(x'_j, x'_{i+1}) + d(x_j, x'_{i+1})]\\
    d^{\leq i}_-  & = \sum_{v \in V_-(x'_{i+1}, x'_{i})} d(v, x'_{i+1})
    & = \sum_{j \leq i, y_j = -1 }[d(x'_j, x'_{i+1}) + d(x_j, x'_{i+1})].
\end{align*}

Consider $i = 1, ...., n-1$. If $y_i = -1$ and $y_{i+1}=+1$, we then have 
\begin{equation*}
    f(x'_i, x'_{i+1}) = d^{>i}_- + d^{\leq i}_+.
\end{equation*}
Otherwise, $y_i = +1$ and $y_{i+1}=-1$, then 
\begin{equation*}
    f(x'_i, x'_{i+1}) = d^{>i}_+ + d^{\leq i}_-.
\end{equation*}

Therefore, to compute the total noise function, we should have an efficient method to compute $(d^{>i}_+, d^{>i}_-, d^{\leq i}_+, d^{\leq i}_-)$. This can be achived using the dynamic updates.

The following proposition claims that how the information of $N^{\leq i}_-$ and $d^{\leq i}_-$ can be used to derive  $N^{\leq i+1}_-$ and $d^{\leq i+1}_-$ efficiently. The proof of the proposition will show the procedure in details.

\begin{proposition}
    For $i=1,..., n-1$, if we have $N^{\leq i}_-$ and $d^{\leq i}_-$, then we can update $N^{\leq i+1}_-$ and $d^{\leq i+1}_-$ in constant time.
\end{proposition}
\begin{proof}
    By definition,
    \[
        d^{\leq i}_- = \sum_{\substack{j \le i \\ y_j=-1}} \big( d(x'_j, x'_{i+1}) + d(x_j, x'_{i+1}) \big),\qquad
        d^{\leq i+1}_- = \sum_{\substack{j \le i+1 \\ y_j=-1}} \big( d(x'_j, x'_{i+2}) + d(x_j, x'_{i+2}) \big).
    \]
    First consider the sum over indices $j \le i$ with $y_j=-1$. Due to the tree structure consisting of the spine (connecting the $x'_k$ in order) and spokes (connecting each $x_k$ to its projection $x'_k$), for all $j \le i$, the unique path from $x'_j$ (or $x_j$) to $x'_{i+2}$ passes through $x'_{i+1}$. Therefore,
    \[
        d(x'_j, x'_{i+2}) = d(x'_j, x'_{i+1}) + d(x'_{i+1}, x'_{i+2}),\qquad
        d(x_j,  x'_{i+2}) = d(x_j,  x'_{i+1}) + d(x'_{i+1}, x'_{i+2}).
    \]
    Summing over all $j \le i$ with $y_j=-1$, the increase due to shifting the anchor from $x'_{i+1}$ to $x'_{i+2}$ is
    \[
        N^{\le i}_- \cdot d(x'_{i+1}, x'_{i+2}),
    \]
    since $N^{\le i}_-$ counts exactly the number of negative vertices participating in the sum (each index $j$ contributes two vertices $x'_j$ and $x_j$).

    Additionally, if $y_{i+1}=-1$, two new vertices appear in the region $\le i+1$, namely $x'_{i+1}$ and $x_{i+1}$. Their contribution to $d^{\le i+1}_-$ is
    \[
        d(x'_{i+1}, x'_{i+2}) + d(x_{i+1}, x'_{i+2})
        = d(x'_{i+1}, x'_{i+2}) + \big( d(x_{i+1}, x'_{i+1}) + d(x'_{i+1}, x'_{i+2}) \big)
        = 2\,d(x'_{i+1}, x'_{i+2}) + d(x_{i+1}, x'_{i+1}).
    \]
    Combining both parts, we obtain the update formulas for the two cases based on the label $y_{i+1}$:
    \begin{align*}
        N^{\le i+1}_- &= \begin{cases}
            N^{\le i}_- + 2, & \text{if } y_{i+1}=-1,\\
            N^{\le i}_-, & \text{if } y_{i+1}=+1,
        \end{cases}\\[4pt]
        d^{\le i+1}_- &= \begin{cases}
            d^{\le i}_- + N^{\le i}_-\, d(x'_{i+1}, x'_{i+2}) + 2\,d(x'_{i+1}, x'_{i+2}) + d(x_{i+1}, x'_{i+1}), & \text{if } y_{i+1}=-1,\\
            d^{\le i}_- + N^{\le i}_-\, d(x'_{i+1}, x'_{i+2}), & \text{if } y_{i+1}=+1.
        \end{cases}
    \end{align*}
    Clearly, each update step only uses quantities known at step $i$ and distances between consecutive spine points, so it can be performed in constant time. Therefore, knowing $N^{\le i}_-$ and $d^{\le i}_-$ is sufficient to update $N^{\le i+1}_-$ and $d^{\le i+1}_-$.
\end{proof}

Extending above proposition, we obtain the following result.
\begin{proposition}
    We can compute the following quantities in $O(n)$.
    \begin{enumerate}
        \item $(N^{>i}_+, N^{>i}_-, N^{\leq i}_+, N^{\leq i}_-)$ for all $i=1,..., n-1$
        \item $(d^{>i}_+, d^{>i}_-, d^{\leq i}_+, d^{\leq i}_-)$ for all $i=1,..., n-1$
        \item $f(x'_i, x'_{i+1})$ for all $i=1,..., n-1$.
    \end{enumerate}
\end{proposition}
\begin{proof}
    We prove this by applying the Proposition XXX in a systematic manner, using the dynamic programming technique on trees.

    \textit{Step 1: Computing all counting quantities in $O(n)$.}
    The counting quantities can be computed by simple linear scans:
    \begin{itemize}
        \item $N^{>i}_+, N^{>i}_-$: Scan from right to left, counting positive/negative labels.
        \item $N^{\leq i}_+, N^{\leq i}_-$: Scan from left to right, counting positive/negative labels.
    \end{itemize}
    Each quantity is updated in $O(1)$ per position, giving total $O(n)$ time.

    \textit{Step 2: Computing all distance quantities in $O(n)$.}
    This is where we apply Proposition XXX. The key insight is that our augmented tree has a spine structure, which allows efficient re-rooting operations.

    Left-to-right pass for computing $d^{\leq i}_+, d^{\leq i}_-$:\remLE{check index carefully}
    \begin{itemize}
        \item Initialize: $d^{\leq 1}_+ = 2d(x'_1, x'_2) + d(x_1, x'_2)$ if $y_1 = +1$, else $0$.
        \item For $i = 1, \ldots, n-2$: Apply Proposition 1 (and its symmetric version for positive labels) to update $d^{\leq i+1}_\pm$ from $d^{\leq i}_\pm$ in $O(1)$ time.
    \end{itemize}

    Right-to-left pass for computing $d^{>i}_+, d^{>i}_-$:
    \begin{itemize}
        \item Initialize: $d^{>n-1}_+ = 0, d^{>n-1}_- = 0$ (no points beyond position $n-1$).
        \item For $i = n-1, n-2, \ldots, 2$: Use the symmetric update formula to compute $d^{>(i-1)}_\pm$ from $d^{>i}_\pm$ in $O(1)$ time.
    \end{itemize}

    The symmetric update formula for the right-to-left pass is obtained by reversing the roles: when moving from anchor $x'_{i+1}$ to $x'_i$, all points $j > i$ have their distances increased by $d(x'_i, x'_{i+1})$, and if $y_i = -1$, we add the contribution of the new point $i$.

    \textit{Step 3: Computing $f(x'_i, x'_{i+1})$ in $O(n)$.}
    Once all counting and distance quantities are computed, we can evaluate $f(x'_i, x'_{i+1})$ for each $i = 1, \ldots, n-1$ using the given formulas in XXX.
    % \begin{itemize}
    %     \item If $y_i = -1, y_{i+1} = +1$: $f(x'_i, x'_{i+1}) = d^{>i}_- + d^{\leq i}_+$.
    %     \item If $y_i = +1, y_{i+1} = -1$: $f(x'_i, x'_{i+1}) = d^{>i}_+ + d^{\leq i}_-$.
    % \end{itemize}
    Each evaluation takes $O(1)$ time, giving total $O(n)$ time.

    % \textbf{Conclusion:} By Proposition 5.3 (dynamic programming on trees), the re-rooting operations along the spine can be performed efficiently. Since each of the three steps requires $O(n)$ time, the total time complexity is $O(n)$.
\end{proof}



% \subsubsection{Dynamic Programming for Objective Evaluation}

% To efficiently compute $f(u,v)$ for any pair $(u,v)$, we employ a dynamic programming approach using two depth-first search (DFS) passes along the branch $u \to v$:

% \begin{enumerate}
% \item \textbf{Size computation pass}: Root the tree $T$ at vertex $u$ and compute, for every node $a$, the subtree sizes:
% \begin{align}
% \mathrm{sz}_+(a) &= |\{z \in \text{subtree}(a) : y(z) = +1\}|, \\
% \mathrm{sz}_-(a) &= |\{z \in \text{subtree}(a) : y(z) = -1\}|.
% \end{align}

% \item \textbf{Distance accumulation pass}: Still with the tree rooted at $u$, accumulate the weighted distances:
% \begin{align}
% \mathrm{dist}_+(a) &= \sum_{\substack{z \in \text{subtree}(a)\\ y(z) = +1}} d(z,a), \\
% \mathrm{dist}_-(a) &= \sum_{\substack{z \in \text{subtree}(a)\\ y(z) = -1}} d(z,a).
% \end{align}

% These values are then combined along the unique path $u \to v$ to obtain $f_+(u,v)$. By symmetry, rooting the tree at $v$ yields $f_-(v,u)$.
% \end{enumerate}

% Since our augmented tree has a spine-and-spoke structure (essentially a path with leaves), all tree traversals can be performed in linear time.

\subsubsection{Algorithm and Complexity Analysis}


\begin{algorithm}[htbp]
\caption{SVM on Tree}
\label{alg:svm_tree}
\begin{algorithmic}[1]
\STATE \textbf{Input:} Labeled dataset $\{(x_i,y_i)\}_{i=1}^n \subset \mathbb{R}^d \times \{-1,+1\}$
\STATE \textbf{Output:} Optimal support pair $(s^*,p^*)$

\STATE Compute class means $\mu_-$, $\mu_+$ and spine direction $w$
\STATE Project all points: $x_i' = \mu_- + \langle x_i-\mu_-, w\rangle w$ for $i = 1,\ldots,n$
\STATE Sort projections $\{x_i'\}$ by coordinate $t_i = \langle x_i-\mu_-, w\rangle$
\STATE Construct augmented tree $T$ with spoke and spine edges (see XXX)
\STATE Initialize $L_{\min} = \infty$ %, $(s^*,p^*) = \emptyset$
\STATE Compute $f(x'_i, x'_{i+1})$ for all $i=1,..., n-1$ using preprocessing step in XXX
\STATE Compute the total loss $L_1(x'_i, x'_{i+1}) = f(x'_i, x'_{i+1}) -d(x'_i, x'_{i+1})$ for all $i=1,..., n-1$
\STATE Let $(s^*,p^*)=(x'_i, x'_{i+1})$ where $(x'_i, x'_{i+1})$ is  the pair with minimum total loss $L_1(x'_i, x'_{i+1})$ and $y_i\neq y_{i+1}$
% \FOR{$i=1,..., n-1$}
%     \STATE Let $(s,p) = (x'_i, x'_{i+1})$ with $y_i \neq y_{i+1}$
%     % \STATE Compute $f(s,p)$ using XXX and XXX
%     \STATE Calculate $L(s,p) = f(s,p) - d(s,p)$
%     \IF{$L(s,p) < L_{\min}$}
%         \STATE $L_{\min} = L(s,p)$
%         \STATE $(s^*,p^*) = (s,p)$
%     \ENDIF
% \ENDFOR

\STATE \textbf{return} $(s^*,p^*)$ % and perpendicular bisector of segment $s^*p^*$
\end{algorithmic}
\end{algorithm}


The computational complexity of our algorithm is analyzed as follows:

\begin{itemize}
\item \textbf{Projection step}: Computing projections for $n$ points in $\mathbb{R}^d$ requires $O(nd)$ operations.
\item \textbf{Sorting step}: Sorting $n$ projections by their coordinates takes $O(n \log n)$ time.
\item \textbf{Tree construction}: Building the augmented tree with $n$ spoke edges and $O(n)$ spine edges requires $O(n)$ time.
\item \textbf{Objective evaluation}: Each step XXX takes $O(n)$ time, and there are $O(n)$ adjacent pairs to evaluate due to Theorem~\ref{thm:adjacency}.
\end{itemize}

We finally obtain
\begin{theorem}[Complexity]
The overall time complexity of Algorithm~\ref{alg:svm_tree} is $O(nd + n \log n)$, or $O(n \log n)$ for fixed dimension $d$.
\end{theorem}

This represents a significant improvement over exhaustive search, which would require $O(n^2)$ \remLE{check} pair evaluations, each taking $O(n)$ time for objective computation.

% \subsubsection{Optimization Problem Statement}

% The core optimization problem solved by our algorithm is:

% \begin{problem}[SVM on Tree]
% Given the augmented tree $T$ and loss function $L(\cdot,\cdot)$, find:
% \[
% (s^*,p^*) = \arg\min_{(s,p): y(s) \neq y(p)} L(s,p) = \arg\min_{(s,p): y(s) \neq y(p)} [f(s,p) - d(s,p)].
% \]
% \end{problem}

% By Theorem~\ref{thm:adjacency}, this optimization reduces to considering only adjacent opposite-label projection pairs on the spine, dramatically reducing the search space from $O(n^2)$ to $O(n)$ candidates.
